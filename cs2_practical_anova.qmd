---
title: "ANOVA"
format:
  html:
    toc: true
    include-after-body:
      - sync-tabs.html
---

```{r}
#| echo: false
#| message: false
#| results: hide
source(file = "setup.R")
```

```{python}
#| echo: false
#| message: false
from plotnine import *
from scipy import stats
import pandas as pd
import pingouin as pg
```

::: {.callout-tip}
## Learning outcomes

**Questions**

- How do I analyse multiple samples of continuous data?
- What is an ANOVA?
- How do I check for differences between groups?

**Objectives**

- Be able to perform an ANOVA in R
- Understand the ANOVA output and evaluate the assumptions
- Understand what post-hoc testing is and how to do this in R
:::

## Purpose and aim
Analysis of variance or ANOVA is a test than can be used when we have multiple samples of continuous response data. Whilst it is possible to use ANOVA with only two samples, it is generally used when we have three or more groups. It is used to find out if the samples came from parent distributions with the same mean. It can be thought of as a generalisation of the two-sample Student’s t-test.

## Libraries and functions

::: {.callout-note collapse="true"}
## Click to expand
::: {.panel-tabset}
## tidyverse

| Libraries            | Description                                                                                                       |
|:-----------------------|:-----------------------------------------------|
| `library(tidyverse)` | A collection of R packages designed for data science                                                              |
| `library(rstatix)`   | Converts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we'll use. |
|`library(ggResidpanel)`|Creates diagnostic plots using `ggplot2`|

|Library| Function| Description|
|:------------------------|:-------------------------------|:---|
|`rstatix`| `get_summary_stats()` | Computes summary statistics|
|`ggResidpanel`|`resid_panel()`| Creates diagnostic plots|

## R

| Function            | Description                                                                                                        |
|:-----------------------------------|:-----------------------------------|
|`lm()`| Fits a linear model |
|`anova()`| Carries out an ANOVA on a linear model ||
| `qqnorm()`| Plots a Q-Q plot for comparison with a normal distribution.|
| `qqline()`| Adds a comparison line to the Q-Q plot.|
| `shapiro.test()`| Performs a Shapiro-Wilk test for normality.|

## Python

| Libraries     | Description                                       |
|:--------------|:--------------------------------------------------|
| `plotnine`    | The Python equivalent of `ggplot2`.               |
| `pandas`      | A Python data analysis and manipulation tool.     |
| `scipy.stats` | A Python module containing statistical functions. |
|`statsmodels`  | A Python module for statistical models, conducting tests and statistical data exploration. |

| Functions                       | Description                                                             |
|:-----------------------------------|:-----------------------------------|
| `pandas.DataFrame.read_csv`     | Reads in a `.csv` file                                                  |
| `pandas.DataFrame.head()`       | Plots the first few rows                                                |
| `pandas.DataFrame.describe()`   | Gives summary statistics                                                |
| `pandas.DataFrame.groupby()`    | Group DataFrame using a mapper or by a Series of columns                |
| `pandas.DataFrame.query()`      | Query the columns of a DataFrame with a boolean expression              |
| `scipy.stats.levene()`          | Performs Levene's test for equality of variance|
:::
:::

## Data and hypotheses
For example, suppose we measure the feeding rate of oyster catchers (shellfish per hour) at three sites characterised by their degree of shelter from the wind, imaginatively called `exposed` (E), `partially sheltered` (P) and `sheltered` (S). We want to test whether the data support the hypothesis that feeding rates don’t differ between locations. We form the following null and alternative hypotheses:

-	$H_0$: The mean feeding rates at all three sites is the same $\mu E = \mu P = \mu S$
-	$H_1$: The mean feeding rates are not all equal.

We will use a one-way ANOVA test to check this.

-	We use a **one-way** ANOVA test because we only have one predictor variable (the categorical variable location).
-	We’re using **ANOVA** because we have more than two groups and we don’t know any better yet with respect to the exact assumptions.

The data are stored in the file `data/CS2-oystercatcher.csv`.

## Summarise and visualise

::: {.panel-tabset}
## tidyverse
First we read in the data.
```{r, message=FALSE, warning=FALSE}
# load data
oystercatcher <- read_csv("data/CS2-oystercatcher-feeding.csv")

# and have a look
oystercatcher
```

The `oystercatcher` data set contains two columns:

1. a `site` column with information on the amount of shelter of the feeding location
2. a `feeding` column containing feeding rates

Next, we get some basic descriptive statistics:

```{r}
# get some basic descriptive statistics
oystercatcher %>% 
  group_by(site) %>% 
  get_summary_stats(type = "common")
```

Finally, we plot the data by `site`:

```{r}
# plot the data
oystercatcher %>% 
  ggplot(aes(x = site, y = feeding)) +
  geom_boxplot()
```
## R
First we read in the data.

```{r}
# load data
oystercatcher_r <- read.csv("data/CS2-oystercatcher-feeding.csv")

# have a look
head(oystercatcher_r)
```

The `oystercatcher` data set contains two columns:

1. a `site` column with information on the amount of shelter of the feeding location
2. a `feeding` column containing feeding rates

Next, we get some basic descriptive statistics. We have three groups, so to get the summary statistics by group we do the following:

```{r}
aggregate(feeding ~ site,
          data = oystercatcher_r,
          summary)
```

Finally, we plot the data by `site`:

```{r}
# plot the data by site
boxplot(feeding ~ site,
        data = oystercatcher_r)
```

## Python
First, we read in the data.

```{python}
# load the data
oystercatcher_py = pd.read_csv("data/CS2-oystercatcher-feeding.csv")

# and have a look
oystercatcher_py.head()
```

The `oystercatcher_py` data set contains two columns:

1. a `site` column with information on the amount of shelter of the feeding location
2. a `feeding` column containing feeding rates

Next, we get some basic descriptive statistics *per group*. Here we use the `pd.groupby()` function to group by `site`. We only want to have summary statistics for the `feeding` variable, so we specify that as well:

```{python}
oystercatcher_py.groupby("site")["feeding"].describe()
```

Finally, we plot the data:

```{python}
#| results: hide
# plot the data
(
  ggplot(oystercatcher_py,
    aes(x = "site", y = "feeding"))
    + geom_boxplot()
)
```

:::

Looking at the data, there appears to be a noticeable difference in feeding rates between the three sites. We would probably expect a reasonably significant statistical result here.

## Assumptions
To use an ANOVA test, we have to make three assumptions:

1.	The parent distributions from which the samples are taken are normally distributed
2.	Each data point in the samples is independent of the others
3.	The parent distributions should have the same variance

In a similar way to the two-sample tests we will consider the normality and equality of variance assumptions both using tests and by graphical inspection (and ignore the independence assumption).

### Normality

First we perform a Shapiro-Wilk test on each site separately.

::: {.panel-tabset}
## tidyverse

We take the data, and `group_by()` site:
```{r}
#| results: hide
# Shapiro-Wilk test on each site
oystercatcher %>% 
  group_by(site) %>% 
  shapiro_test(feeding)
```
## R
Unstack the data and perform a Shapiro-Wilk test on each group separately.

```{r}
# create a new object (a list) that contains the unstacked data
uns_oystercatcher <- unstack(oystercatcher_r,
                             form = feeding ~ site)
# have a look at the data
head(uns_oystercatcher)
```

Next, we perform the Shapiro-Wilk test on each group:
```{r}
shapiro.test(uns_oystercatcher$exposed)
shapiro.test(uns_oystercatcher$partial)
shapiro.test(uns_oystercatcher$sheltered)
```

## Python

We use the `pg.normality()` function to calculate the statistic. This requires:

* the `dv` dependent variable (`feeding` in our case)
* the `group` variable (`site`)
* and some data

```{python}
pg.normality(dv = "feeding",
             group = "site",
             data = oystercatcher_py)
```
:::

We can see that all three groups appear to be normally distributed which is good.

For ANOVA however, considering each group in turn is often considered quite excessive and, in most cases, it is sufficient to consider the normality of the combined set of _residuals_ from the data. We’ll explain residuals properly in the [next session](#cs3-intro) but effectively they are the difference between each data point and its group mean. The residuals can be obtained directly from a linear model fitted to the data.

So, we create a linear model, extract the residuals and check their normality:

::: {.panel-tabset}
## tidyverse

```{r}
# define the model
lm_oystercatcher <- lm(feeding ~ site,
                       data = oystercatcher)

# extract the residuals
resid_oyster <- residuals(lm_oystercatcher)

# perform Shapiro-Wilk test on residuals
resid_oyster %>% 
  shapiro_test()
```

## R
```{r}
# define the model
lm_oystercatcher_r <- lm(feeding ~ site,
                         data = oystercatcher_r)

# extract the residuals
resid_oyster_r <- residuals(lm_oystercatcher_r)

# perform Shapiro-Wilk test on residuals
shapiro.test(resid_oyster_r)
```
## Python
Unfortunately `pingouin` does not have a straightforward way of extracting residuals (if you know more, please raise an issue!).

So we need to do some statistical gymnastics to get the residuals. I would suggest that you don't worry about the code, just run it and look at the output!

::: {.callout-note collapse=True}
## Technical details (optional)
For this to work in Python, we need to take our predictor variable (`site`) and convert it to a [dummy variable](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html).

We also need to import the `statsmodels.api` module, which contains an `OLS()` function (Ordinary Least Squares - the equivalent of the `lm()` function in R).

We also import `formula.api` so we can use the formula notation in our linear model. We define the formula as `formula= "feeding ~ C(site)"` with `C` conveying that the `site` variable is a category. Lastly we can `.fit()` the model.

If you're familiar with this stuff then you can look at the model itself by running `summary(lm_oystercatcher_py)`. But we'll cover all of this in later sessions.
:::

Eventually, we get the residuals from the linear model with `.resid`. We give this to the `stats.shapiro()` function and off we go...

```{python}
import statsmodels.api as sm
import statsmodels.formula.api as smf

# fitting the model

lm_oystercatcher_py = smf.ols(formula= "feeding ~ C(site)", data = oystercatcher_py).fit()

stats.shapiro(lm_oystercatcher_py.resid)
```

```{python}
import statsmodels.formula.api as smf

# fitting the model

model = smf.ols(formula= "feeding ~ C(site)", data = oystercatcher_py)
result = model.fit()

result.summary()
```
:::

Again, we can see that the combined residuals from all three groups appear to be normally distributed (which is as we would have expected given that they were all normally distributed individually!)

### Equality of Variance

We now test for equality of variance using Bartlett’s test (since we’ve just found that all of the individual groups are normally distributed).

Perform Bartlett’s test on the data:

::: {.panel-tabset}
## tidyverse

```{r}
# check equality of variance
bartlett.test(feeding ~ site,
              data = oystercatcher)
```

Where the relevant p-value is given on the 3rd line. Here we see that each group appears to have the same variance.

## R
```{r}
# check equality of variance
bartlett.test(feeding ~ site,
              data = oystercatcher)
```

Where the relevant p-value is given on the 3rd line. Here we see that each group appears to have the same variance.

## Python
We use the `homoscedasticity()` function from `pingouin` (homoscedasticity is another way of describing equality of variance). The default `method` is `levene`, so we need to specify that we want to use `bartlett`.

```{python}
pg.homoscedasticity(dv = "feeding",
                    group = "site",
                    method = "bartlett",
                    data = oystercatcher_py)
```

Where the relevant p-value is given in the `pval` column. Here we see that each group appears to have the same variance.
:::

### Graphical interpretation and diagnostic plots

R provides a convenient set of graphs that allow us to assess these assumptions graphically.

::: {.panel-tabset}
## tidyverse
In the first session we already created diagnostic Q-Q plots directly from our data, using `stat_qq()` and `stat_qq_line()`. For more specific plots this becomes a bit cumbersome. There is an option to create ggplot-friendly diagnostic plots, using the `ggResidPanel` package.

Let's create the diagnostic plots we're interested in using `ggResidPanel`:

```{r}
#| message: false
lm_oystercatcher %>% 
    resid_panel(plots = c("resid", "qq", "ls", "cookd"),
              smoother = TRUE)
```

*	The top left graph plots the **Residuals plot**. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot looks pretty good.
*	The top right graph shows the **Q-Q plot** which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.
*	The bottom left **Location-scale** graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.
*	The last graph shows the **Cook's distance** and tests if any one point has an unnecessarily large effect on the fit. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at `4/n`, with `n` being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.  

## R

We can create the default diagnostic plots in base R by simply plotting the `lm` object.

```{r}
# create a neat 2x2 window
par(mfrow = c(2,2))
# create the diagnostic plots
plot(lm_oystercatcher)
# and return the window back to normal
par(mfrow = c(1,1))
```

The second line creates four diagnostic plots.

*	The top left graph plots the **Residuals vs Fitted**. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal red line. This plot looks pretty good.
*	The top right graph shows the **Normal Q-Q plot** which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This plot looks good.
*	The bottom left **Scale-Location** graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.
* The bottom right **Residuals vs Factor Levels** plot shows the residuals for each group (= site). This plot is only displayed if there are equal number of observations in each group and we'll explain more about this in the next session.
:::

We can see that these graphs are very much in line with what we’ve just looked at using the test, which is reassuring. The groups all appear to have the same spread of data, and the Q-Q plot shows that the assumption of normality is alright.

::: {.callout-important}
## Assessing assumptions
At this stage, I should point out that I nearly always stick with the graphical method for assessing the assumptions of a test. Assumptions are rarely either completely met or not met and there is always some degree of personal assessment.

Whilst the formal statistical tests (like Shapiro-Wilk) are technically fine, they can often create a false sense of things being absolutely right or wrong in spite of the fact that they themselves are still probabilistic statistical tests. In these exercises we are using both approaches whilst you gain confidence and experience in interpreting the graphical output and whilst it is absolutely fine to use both in the future I would strongly recommend that you don’t rely solely on the statistical tests in isolation.
:::

## Implement and interpret the test
Perform an ANOVA test on the data:

::: {.panel-tabset}
## tidyverse
```{r}
anova(lm_oystercatcher)
```

This takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we've called `lm_oystercatcher`, but which you can call what you like).

In the output:

-	The 1st line just tells you the that this is an ANOVA test
-	The 2nd line tells you what the response variable is (in this case feeding)
-	The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:
    - The `Df` column contains the degrees of freedom values on each row, `r glance(lm_oystercatcher) %>% pull(df) %>% as.numeric()` and `r glance(lm_oystercatcher) %>% pull(df.residual) %>% as.numeric()` (which we can use for the reporting)
    - The `F` value column contains the F statistic, `r round(glance(lm_oystercatcher) %>% pull(statistic) %>% as.numeric(), digits = 2)` (which again we’ll need for reporting).
    - The p-value is 2.2e-16 and is the number directly under the `Pr(>F)` on the 4th line (to be precise, it is `r formatC((glance(lm_oystercatcher) %>% pull(p.value) %>% as.numeric()), format = "e", digits = 2)` but anything smaller than 2.2e-16 gets reported as `< 2.2e-16`).
    - The other values in the table (in the `Sum Sq` and `Mean Sq`) columns are used to calculate the F statistic itself and we don’t need to know these.
-	The 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a *** symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a * character next to it, etc. Thankfully we can all cope with actual numbers and don’t need a short-hand code to determine the reporting of our experiments (please tell me that’s true…!)

## R

```{r}
anova(lm_oystercatcher_r)
```

This takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we've called `lm_oystercatcher_r`, but which you can call what you like).

In the output:

-	The 1st line just tells you the that this is an ANOVA test
-	The 2nd line tells you what the response variable is (in this case feeding)
-	The 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:
- The `Df` column contains the degrees of freedom values on each row, `r glance(lm_oystercatcher_r) %>% pull(df) %>% as.numeric()` and `r glance(lm_oystercatcher_r) %>% pull(df.residual) %>% as.numeric()` (which we can use for the reporting)
- The `F` value column contains the F statistic, `r round(glance(lm_oystercatcher_r) %>% pull(statistic) %>% as.numeric(), digits = 2)` (which again we’ll need for reporting).
- The p-value is 2.2e-16 and is the number directly under the `Pr(>F)` on the 4th line (to be precise, it is `r formatC((glance(lm_oystercatcher_r) %>% pull(p.value) %>% as.numeric()), format = "e", digits = 2)` but anything smaller than 2.2e-16 gets reported as `< 2.2e-16`).
- The other values in the table (in the `Sum Sq` and `Mean Sq`) columns are used to calculate the F statistic itself and we don’t need to know these.
-	The 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a *** symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a * character next to it, etc. Thankfully we can all cope with actual numbers and don’t need a short-hand code to determine the reporting of our experiments (please tell me that’s true…!)

## Python

There are different ways of conducting an ANOVA in Python, with `scipy.stats` [proving an option](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html).
However, I find that [anova()](https://pingouin-stats.org/generated/pingouin.anova.html#pingouin.anova) function in `pingouin` provides the easiest and most-detailed option to do this.

It takes the following arguments:

* `dv`: dependent variable (response variable; in our case `feeding`)
* `between`: between-subject factor (predictor variable; in our case `site`)
* `data`: which function doesn't!?
* `detailed`: optional `True` or `False`, we're setting it to `True` because we like to know what we're doing!

```{python}
pg.anova(dv = "feeding",
         between = "site",
         data = oystercatcher_py,
         detailed = True)
```

This creates a linear model based on the data, _i.e_. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis.

In the output:

* `Source`: Factor names - in our case these are the different sites (`site`)
* `SS`: Sums of squares (we'll get to that in a bit)
* `DF`: Degrees of freedom (at the moment only used for reporting)
* `MS`: Mean squares
* `F`: Our F-statistic
* `p-unc`: p-value (`unc` stands for "uncorrected" - more on multiple testing correction later)
* `np2`: Partial eta-square effect sizes (more on this later)
:::

Again, the p-value is what we’re most interested in here and shows us the probability of getting samples such as ours if the null hypothesis were actually true.

Since the p-value is very small (much smaller than the standard significance level of 0.05) we can say "that it is very unlikely that these three samples came from the same parent distribution" and as such we can reject our null hypothesis and state that:

> A one-way ANOVA showed that the mean feeding rate of oystercatchers differed significantly between locations (F = `r round(glance(lm_oystercatcher) %>% pull(statistic) %>% as.numeric(), digits = 2)`, df = `r glance(lm_oystercatcher) %>% pull(df) %>% as.numeric()`, `r glance(lm_oystercatcher) %>% pull(df.residual) %>% as.numeric()`, p = `r formatC((glance(lm_oystercatcher) %>% pull(p.value) %>% as.numeric()), format = "e", digits = 2)`).
