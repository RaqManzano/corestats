[
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html",
    "title": "Wilcoxon signed rank test",
    "section": "",
    "text": "A Wilcoxon signed-rank test is an alternative to a paired t-test. It does not require that the data are drawn from normal distributions, but it does require that the distribution of the differences is symmetric. We’re effectively testing to see if the median of the differences between the two samples differs significantly from zero."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html#libraries-and-functions",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html#libraries-and-functions",
    "title": "Wilcoxon signed rank test",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(tidyverse)\nA collection of R packages designed for data science\n\n\nlibrary(rstatix)\nConverts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we’ll use.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nrstatix::t_test()\nPerforms a one-sample t-test, Student’s t-test and Welch’s t-test in later sections.\n\n\nrstatix::shapiro_test()\nPerforms a Shapiro-Wilk test for normality.\n\n\nggplot2::stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nggplot2::stat_qq_line()\nAdds a comparison line to the Q-Q plot.\n\n\nggplot2::geom_jitter()\nPlots jittered points by adding a small amount of random variation to each point, to handle overplotting\n\n\ntidyr::pivot_wider()\n“Widens” the data, increasing the number of columns\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nstats::reshape\nReshapes a data frame from ‘long’ to ‘wide’ and vice versa\n\n\nt.test()\nPerforms a one-sample t-test, Student’s t-test and Welch’s t-test in later sections.\n\n\nqqnorm()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nqqline()\nAdds a comparison line to the Q-Q plot.\n\n\nshapiro.test()\nPerforms a Shapiro-Wilk test for normality.\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\nscipy.stats\nA Python module containing statistical functions.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npandas.DataFrame.read_csv\nReads in a .csv file\n\n\nscipy.stats.wilcoxon()\nTests the null hypothesis that two related paired samples come from the same distribution.\n\n\nscipy.stats.shapiro()\nPerforms the Shapiro-Wilk test\n\n\nplotnine.stats.stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nplotnine.stats.stat_qq_line()\nAdds a comparison line to the Q-Q plot."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html#data-and-hypotheses",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html#data-and-hypotheses",
    "title": "Wilcoxon signed rank test",
    "section": "Data and hypotheses",
    "text": "Data and hypotheses\nUsing the cortisol dataset from before we form the following null and alternative hypotheses:\n\n\\(H_0\\): The median of the difference in cortisol levels between the two groups is 0 \\((\\mu M = \\mu E)\\)\n\\(H_1\\): The median of the difference in cortisol levels between the two groups is not 0 \\((\\mu M \\neq \\mu E)\\)\n\nWe use a two-tailed Wilcoxon signed-rank test to see if we can reject the null hypothesis."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html#summarise-and-visualise",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html#summarise-and-visualise",
    "title": "Wilcoxon signed rank test",
    "section": "Summarise and visualise",
    "text": "Summarise and visualise\nAlready implemented previously."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html#assumptions",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html#assumptions",
    "title": "Wilcoxon signed rank test",
    "section": "Assumptions",
    "text": "Assumptions\nThese have been checked previously."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html#implement-and-interpret-the-test",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html#implement-and-interpret-the-test",
    "title": "Wilcoxon signed rank test",
    "section": "Implement and interpret the test",
    "text": "Implement and interpret the test\nPerform a two-tailed, Wilcoxon signed-rank test:\n\ntidyverseRPython\n\n\n\n# perform the test\ncortisol %>% \n  wilcox_test(cortisol ~ time,\n              alternative = \"two.sided\",\n              paired = TRUE)\n\n# A tibble: 1 × 7\n  .y.      group1  group2     n1    n2 statistic        p\n* <chr>    <chr>   <chr>   <int> <int>     <dbl>    <dbl>\n1 cortisol evening morning    20    20        13 0.000168\n\n\n\nThe first argument gives the formula\nThe second argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe third argument says that the data are paired\n\n\n\n\nwilcox.test(cortisol ~ time,\n            alternative = \"two.sided\",\n            paired = TRUE,\n            data = cortisol_r)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  cortisol by time\nV = 13, p-value = 0.0001678\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nThe first argument gives the formula\nThe second argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe third argument indicates that the test is paired\nThe last argument is the data set\n\n\n\nWe’ll use the wide format data set that we created previously:\n\nstats.wilcoxon(cortisol_diff_py[\"evening\"],\n               cortisol_diff_py[\"morning\"],\n               alternative = \"two-sided\")\n\nWilcoxonResult(statistic=13.0, pvalue=0.0001678466796875)\n\n\n\n\n\nThe p-value is given in the p column (p-value = 0.000168). Given that this is less than 0.05 we can still reject the null hypothesis.\n\nA two-tailed, Wilcoxon signed-rank test indicated that the median cortisol level in adult females differed significantly between the morning (320.5 nmol/l) and the evening (188.9 nmol/l) (V = 13, p = 0.00017)."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html#exercise-deer-legs",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html#exercise-deer-legs",
    "title": "Wilcoxon signed rank test",
    "section": "Exercise: Deer legs",
    "text": "Exercise: Deer legs\nUsing the following data on deer legs (yes, really!), test the null hypothesis that the fore and hind legs of the deer in this data set are the same length.\n\n\n# A tibble: 10 × 2\n   hindleg foreleg\n     <dbl>   <dbl>\n 1     142     138\n 2     140     136\n 3     144     147\n 4     144     139\n 5     142     143\n 6     146     141\n 7     149     143\n 8     150     145\n 9     142     136\n10     148     146\n\n\nDo these results provide any evidence to suggest that fore- and hind-leg length differ in deer?\n\nWrite down the null and alternative hypotheses\nChoose a tidy representation for the data and create a csv file (I’ll stop asking you to do this from now on…)\nImport the data into R\nSummarise and visualise the data\nCheck your assumptions (normality and variance) using appropriate tests\nDiscuss with your (virtual) neighbour which test is most appropriate?\nPerform the test\nWrite down a sentence that summarises the results that you have found\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : foreleg average (mean or median) \\(=\\) hindleg average (mean or median)\n\\(H_1\\) : foreleg average \\(\\neq\\) hindleg average\n\n\nImport data, summarise and visualise\nFirst of all, we need to get the data into a tidy format (every variable is a column, each observation is a row). Doing this in Excel, and adding a ID gives us:\n\ntidyverseRPython\n\n\n\n# load the data\ndeer <- read_csv(\"data/examples/cs1-deer.csv\")\n\n# have a look\ndeer\n\n# A tibble: 20 × 3\n      id leg     length\n   <dbl> <chr>    <dbl>\n 1     1 hindleg    142\n 2     2 hindleg    140\n 3     3 hindleg    144\n 4     4 hindleg    144\n 5     5 hindleg    142\n 6     6 hindleg    146\n 7     7 hindleg    149\n 8     8 hindleg    150\n 9     9 hindleg    142\n10    10 hindleg    148\n11     1 foreleg    138\n12     2 foreleg    136\n13     3 foreleg    147\n14     4 foreleg    139\n15     5 foreleg    143\n16     6 foreleg    141\n17     7 foreleg    143\n18     8 foreleg    145\n19     9 foreleg    136\n20    10 foreleg    146\n\n\nThe ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an id column, where each observation has a unique ID.\nLet’s look at the data and see what we can see.\n\n# summarise the data\ndeer %>% \n  select(-id) %>% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 1 × 10\n  variable     n   min   max median   iqr  mean    sd    se    ci\n  <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 length      20   136   150    143  5.25  143.  4.01 0.896  1.88\n\n\n\n# or even summarise by leg type\ndeer %>% \n  select(-id) %>% \n  group_by(leg) %>% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  leg     variable     n   min   max median   iqr  mean    sd    se    ci\n  <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 foreleg length      10   136   147    142  6.25  141.  4.03  1.27  2.88\n2 hindleg length      10   140   150    144  5.5   145.  3.40  1.08  2.43\n\n\n\n# we can also visualise the data\ndeer %>% \n  ggplot(aes(x = leg, y = length)) +\n  geom_boxplot()\n\n\n\n\nAll of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have paired data. What we really need to look at is the difference in leg length for each observation:\n\n# create a data set that contains the difference in leg length\nleg_diff <- deer %>% \n  pivot_wider(names_from = leg, values_from = length) %>% \n  mutate(leg_diff = hindleg - foreleg)\n\n\n# plot the difference in leg length\nleg_diff %>% \n  ggplot(aes(y = leg_diff)) +\n  geom_boxplot()\n\n\n\n\nAdditionally, we can also plot the data by observation:\n\n# plot the data by observation\ndeer %>% \n  ggplot(aes(x = leg, y = length, group = id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\ndeer_r <- read.csv(\"data/examples/cs1-deer.csv\")\n\nhead(deer_r)\n\n  id     leg length\n1  1 hindleg    142\n2  2 hindleg    140\n3  3 hindleg    144\n4  4 hindleg    144\n5  5 hindleg    142\n6  6 hindleg    146\n\n\nThe ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an id column, where each observation has a unique ID.\nLet’s look at the data and see what we can see.\n\n# summarise the data\nsummary(deer_r)\n\n       id           leg                length     \n Min.   : 1.0   Length:20          Min.   :136.0  \n 1st Qu.: 3.0   Class :character   1st Qu.:140.8  \n Median : 5.5   Mode  :character   Median :143.0  \n Mean   : 5.5                      Mean   :143.1  \n 3rd Qu.: 8.0                      3rd Qu.:146.0  \n Max.   :10.0                      Max.   :150.0  \n\n\n\n# or even summarise by leg type\naggregate(length ~ leg, data = deer_r, summary)\n\n      leg length.Min. length.1st Qu. length.Median length.Mean length.3rd Qu.\n1 foreleg      136.00         138.25        142.00      141.40         144.50\n2 hindleg      140.00         142.00        144.00      144.70         147.50\n  length.Max.\n1      147.00\n2      150.00\n\n\n\n# we can also visualise the data\nboxplot(length ~ leg, data = deer_r)\n\n\n\n\nAll of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have paired data. What we really need to look at is the difference in leg length for each observation:\n\nleg_diff_r <- reshape(deer_r,\n        idvar = \"id\",\n        timevar = \"leg\",\n        direction = \"wide\")\n\n# calculate difference in leg length\nleg_diff_r$leg_diff <- leg_diff_r$length.hindleg - leg_diff_r$length.foreleg\n\nhead(leg_diff_r)\n\n  id length.hindleg length.foreleg leg_diff\n1  1            142            138        4\n2  2            140            136        4\n3  3            144            147       -3\n4  4            144            139        5\n5  5            142            143       -1\n6  6            146            141        5\n\n\n\n# plot the difference in leg length\nboxplot(leg_diff_r$leg_diff)\n\n\n\n\nAdditionally, we can also plot the data by observation:\n\n# plot the data by observation\nmatplot(t(leg_diff_r[ , 2:3]),\n        pch = 1,\n        type = c(\"b\"),\n        col = 1:10)\n\n\n\n\nAgain, as far as I am aware of, there isn’t a straightforward method of plotting paired data using the base R functionality. Hence the data gymnastics:\n\nthe default plot() function doesn’t support this - the standard matplot() function does\nthe t function transposes the data, and I’m only selecting the second and third columns ([ , 2:3]) which contain the paired leg measurements.\nto group (pair) the data, we’re using colours, one for each of the 10 observations (col = 1:10)\n\n\n\n\n# load the data\ndeer_py = pd.read_csv(\"data/examples/cs1-deer.csv\")\n\n# have a look\ndeer_py.head()\n\n   id      leg  length\n0   1  hindleg     142\n1   2  hindleg     140\n2   3  hindleg     144\n3   4  hindleg     144\n4   5  hindleg     142\n\n\nThe ordering of the data is important here; the first hind leg row corresponds to the first fore leg row, the second to the second and so on. To indicate this we use an id column, where each observation has a unique ID.\nLet’s look at the data and see what we can see.\n\n# summarise the data\ndeer_py.describe()\n\n              id      length\ncount  20.000000   20.000000\nmean    5.500000  143.050000\nstd     2.946898    4.006245\nmin     1.000000  136.000000\n25%     3.000000  140.750000\n50%     5.500000  143.000000\n75%     8.000000  146.000000\nmax    10.000000  150.000000\n\n\nWe can also summarise by leg type, but we have to put our data into a wide format first:\n\n# reformat the data into a 'wide' format\nleg_diff_py = pd.pivot(deer_py, index = \"id\", columns = \"leg\", values = \"length\")\n\n# add a new column with difference between\n# evening and morning cortisol levels\nleg_diff_py[\"leg_diff\"] = leg_diff_py[\"hindleg\"].subtract(leg_diff_py[\"foreleg\"])\n      \n# have a look at the format\nleg_diff_py.head()\n\nleg  foreleg  hindleg  leg_diff\nid                             \n1        138      142         4\n2        136      140         4\n3        147      144        -3\n4        139      144         5\n5        143      142        -1\n\n\nCreate the summary:\n\nleg_diff_py.describe()\n\nleg       foreleg    hindleg   leg_diff\ncount   10.000000   10.00000  10.000000\nmean   141.400000  144.70000   3.300000\nstd      4.033196    3.40098   3.056868\nmin    136.000000  140.00000  -3.000000\n25%    138.250000  142.00000   2.500000\n50%    142.000000  144.00000   4.500000\n75%    144.500000  147.50000   5.000000\nmax    147.000000  150.00000   6.000000\n\n\nWe can visualise this difference:\n\n# we can also visualise the data\n(\n  ggplot(leg_diff_py,\n    aes(x = \"1\",\n        y = \"leg_diff\"))\n  + geom_boxplot()\n)\n\n\n\n\nAll of this suggests that there might be a difference between the legs, with hind legs being longer than forelegs. However, this representation obscures the fact that we have paired data. What we really need to look at is the difference in leg length for each observation:\n\n# plot paired observations\n(\n  ggplot(deer_py,\n    aes(x = \"leg\",\n        y = \"length\",\n        group = \"id\"))\n  + geom_point()\n  + geom_line()\n)\n\n\n\n\n\n\n\nAll of this gives us a much clearer picture. It looks as though the hindlegs are about 4 cm longer than the forelegs, on average. It also suggests that our leg differences might not be normally distributed (the data look a bit skewed in the boxplot).\n\n\nAssumptions\nWe need to consider the distribution of the difference in leg lengths rather than the individual distributions.\n\ntidyverseRPython\n\n\nShapiro-Wilk test:\n\n# perform Shapiro-Wilk test on leg differences\nleg_diff %>% \n  shapiro_test(leg_diff)\n\n# A tibble: 1 × 3\n  variable statistic      p\n  <chr>        <dbl>  <dbl>\n1 leg_diff     0.814 0.0212\n\n\nQ-Q plot:\n\n# create a Q-Q plot\nleg_diff %>% \n  ggplot(aes(sample = leg_diff)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")\n\n\n\n\n\n\nShapiro-Wilk test:\n\n# perform Shapiro-Wilk test on leg differences\nshapiro.test(leg_diff_r$leg_diff)\n\n\n    Shapiro-Wilk normality test\n\ndata:  leg_diff_r$leg_diff\nW = 0.81366, p-value = 0.02123\n\n\nQ-Q plot:\n\n# create a Q-Q plot\nqqnorm(leg_diff_r$leg_diff)\nqqline(leg_diff_r$leg_diff, col = \"red\")\n\n\n\n\n\n\nShapiro-Wilk test:\n\n# perform Shapiro-Wilk test on leg length differences\nstats.shapiro(leg_diff_py[\"leg_diff\"])\n\nShapiroResult(statistic=0.8136557340621948, pvalue=0.021234776824712753)\n\n\nCreate the Q-Q plot:\n\n# create the Q-Q plot\n(\n  ggplot(leg_diff_py,\n    aes(sample = \"leg_diff\"))\n    + stat_qq()\n    + stat_qq_line(colour = \"red\")\n)\n\n\n\n\n\n\n\nBoth our Shapiro-Wilk test and our Q-Q plot suggest that the difference data aren’t normally distributed, which rules out a paired t-test. We should therefore consider a paired Wilcoxon test next. Remember that this test requires that the distribution of differences be symmetric, whereas our box plot from before suggested that the data were very much skewed.\n\n\nConclusions\nSo, frustratingly, neither of the tests at our disposal are appropriate for this data set. The differences in fore leg and hind leg lengths are neither normal enough for a paired t-test nor are they symmetric enough for a Wilcoxon test. We also don’t have enough data to just use the t-test (we’d need more than 30 points or so). So what do we do in this situation? Well, the answer is that there aren’t actually any traditional statistical tests that are valid for this data set as it stands!\nThere are two options available to someone:\n\ntry transforming the raw data (take logs, square root, reciprocals) and hope that one of them leads to a modified data set that satisfies the assumptions of one of the tests we’ve covered, or\nuse a permutation test approach (which would work but is beyond the scope of this course).\n\nThe reason I included this example in the first practical is purely to illustrate how a very simple data set with an apparently clear message (leg lengths differ within deer) can be intractable. You don’t need to have very complex data sets before you go beyond the capabilities of classical statistics.\nAs Jeremy Clarkson would put it:\n\nAnd on that bombshell, it’s time to end. Goodnight!"
  },
  {
    "objectID": "cs1_practical_two-sample-paired-wilcoxon.html#key-points",
    "href": "cs1_practical_two-sample-paired-wilcoxon.html#key-points",
    "title": "Wilcoxon signed rank test",
    "section": "Key points",
    "text": "Key points\n\n\n\n\n\n\nNote\n\n\n\n\nWe use two-sample tests to see if two samples of continuous data come from the same parent distribution\nThis essentially boils down to testing if the mean or median differs between the two samples\nThere are 5 key two-sample tests: Student’s t-test, Welch’s t-test, Mann-Whitney U test, paired t-test and Wilcoxon signed-rank test\nWhich one you use depends on normality of the distribution, sample size, paired or unpaired data and variance of the samples\nParametric tests are used if the data are normally distributed or the sample size is large\nNon-parametric tests are used if the data are not normally distributed and the sample size is small\nEquality of variance then determines which test is appropriate\nYou three questions to determine the test:\n\nis my data paired?\ndo I need a parametric or non-parametric test\ncan I assume equality of variance?"
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html",
    "href": "cs1_practical_two-sample-t-test.html",
    "title": "Student’s t-test",
    "section": "",
    "text": "── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#libraries-and-functions",
    "href": "cs1_practical_two-sample-t-test.html#libraries-and-functions",
    "title": "Student’s t-test",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(tidyverse)\nA collection of R packages designed for data science\n\n\nlibrary(rstatix)\nConverts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we’ll use.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nrstatix::get_summary_stats()\nComputes summary statistics\n\n\nrstatix::levene_test()\nPerform Levene’s test for equality of variance (non-normally distributed data)\n\n\nbartlett.test()\nPerform Bartlett’s test for equality of variance (normally distributed data)\n\n\nggplot2::stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nggplot2::stat_qq_line()\nAdds a comparison line to the Q-Q plot.\n\n\n\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(car)\nCompanion to Applied Regression, provides additional statistical functionality.\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\naggregate()\nSplits the data into subsets, computes summary statistics for each, and returns the result in a convenient form\n\n\nunstack()\nConverts a stacked data frame into an unstacked data frame (or a list if the lengths of the samples are different)\n\n\nbartlett.test()\nPerform Bartlett’s test for equality of variance (normally distributed data)\n\n\ncar::leveneTest()\nPerform Levene’s test for equality of variance (non-normally distributed data)\n\n\nt.test()\nPerforms a one-sample t-test, Student’s t-test and Welch’s t-test in later sections.\n\n\nqqnorm()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nqqline()\nAdds a comparison line to the Q-Q plot.\n\n\nshapiro.test()\nPerforms a Shapiro-Wilk test for normality.\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\nscipy.stats\nA Python module containing statistical functions.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npandas.DataFrame.read_csv\nReads in a .csv file\n\n\npandas.DataFrame.head()\nPlots the first few rows\n\n\npandas.DataFrame.describe()\nGives summary statistics\n\n\npandas.DataFrame.groupby()\nGroup DataFrame using a mapper or by a Series of columns\n\n\npandas.DataFrame.query()\nQuery the columns of a DataFrame with a boolean expression\n\n\nscipy.stats.shapiro()\nPerforms the Shapiro-Wilk test\n\n\nscipy.stats.levene()\nPerforms Levene’s test for equality of variance\n\n\nscipy.stats.bartlett()\nPerforms Bartlett’s test for equality of variance\n\n\nscipy.stats.ttest_ind()\nCalculate the T-test for the means of two independent samples of scores\n\n\nplotnine.stats.stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nplotnine.stats.stat_qq_line()\nAdds a comparison line to the Q-Q plot."
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#data-and-hypotheses",
    "href": "cs1_practical_two-sample-t-test.html#data-and-hypotheses",
    "title": "Student’s t-test",
    "section": "Data and hypotheses",
    "text": "Data and hypotheses\nFor example, suppose we now measure the body lengths of male guppies (in mm) collected from two rivers in Trinidad; the Aripo and the Guanapo. We want to test whether the mean body length differs between samples. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean body length does not differ between the two groups \\((\\mu A = \\mu G)\\)\n\\(H_1\\): The mean body length does differ between the two groups \\((\\mu A \\neq \\mu G)\\)\n\nWe use a two-sample, two-tailed t-test to see if we can reject the null hypothesis.\n\nWe use a two-sample test because we now have two samples.\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) means are different from one another rather than that one mean is specifically bigger or smaller than the other.\nWe’re using Student’s t-test because the sample sizes are big and because we’re assuming that the parent populations have equal variance (We can check this later).\n\nThe data are stored in the file data/CS1-twosample.csv.\nLet’s read in the data and have a quick look at the first rows to see how the data is structured.\nMake sure you have downloaded the data (see: Datasets) and placed it within your working directory.\n\ntidyverseRPython\n\n\nFirst we load the relevant libraries:\n\n# load tidyverse\nlibrary(tidyverse)\n\n# load rstatix, a tidyverse-friendly stats package\nlibrary(rstatix)\n\nWe then read in the data and create a table containing the data.\n\nrivers <- read_csv(\"data/CS1-twosample.csv\")\n\nrivers\n\n# A tibble: 68 × 2\n   river   length\n   <chr>    <dbl>\n 1 Guanapo   19.1\n 2 Guanapo   23.3\n 3 Guanapo   18.2\n 4 Guanapo   16.4\n 5 Guanapo   19.7\n 6 Guanapo   16.6\n 7 Guanapo   17.5\n 8 Guanapo   19.9\n 9 Guanapo   19.1\n10 Guanapo   18.8\n# … with 58 more rows\n\n\n\n\n\nrivers_r <- read.csv(\"data/CS1-twosample.csv\")\n\nhead(rivers_r)\n\n    river length\n1 Guanapo   19.1\n2 Guanapo   23.3\n3 Guanapo   18.2\n4 Guanapo   16.4\n5 Guanapo   19.7\n6 Guanapo   16.6\n\n\n\n\n\nrivers_py = pd.read_csv(\"data/CS1-twosample.csv\")\n\nrivers_py.head()\n\n     river  length\n0  Guanapo    19.1\n1  Guanapo    23.3\n2  Guanapo    18.2\n3  Guanapo    16.4\n4  Guanapo    19.7"
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#cs1-students-sumvisual",
    "href": "cs1_practical_two-sample-t-test.html#cs1-students-sumvisual",
    "title": "Student’s t-test",
    "section": "Summarise and visualise",
    "text": "Summarise and visualise\nLet’s first summarise the data.\n\ntidyverseRPython\n\n\n\nsummary(rivers)\n\n    river               length     \n Length:68          Min.   :11.20  \n Class :character   1st Qu.:18.40  \n Mode  :character   Median :19.30  \n                    Mean   :19.46  \n                    3rd Qu.:20.93  \n                    Max.   :26.40  \n\n\nThis gives us the standard summary statistics, but in this case we have more than one group (Aripo and Guanapo), so it might be helpful to get summary statistics per group. One way of doing this is by using the get_summary_stats() function from the rstatix library.\n\n# get common summary stats for the length column\nrivers %>% \n  group_by(river) %>% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  river   variable     n   min   max median   iqr  mean    sd    se    ci\n  <chr>   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Aripo   length      39  17.5  26.4   20.1   2.2  20.3  1.78 0.285 0.577\n2 Guanapo length      29  11.2  23.3   18.8   2.2  18.3  2.58 0.48  0.983\n\n\nNumbers might not always give you the best insight into your data, so we also visualise our data:\n\nrivers %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nsummary(rivers_r)\n\n    river               length     \n Length:68          Min.   :11.20  \n Class :character   1st Qu.:18.40  \n Mode  :character   Median :19.30  \n                    Mean   :19.46  \n                    3rd Qu.:20.93  \n                    Max.   :26.40  \n\n\nThis gives us the standard summary statistics, but in this case we have more than one group (Aripo and Guanapo), so it might be helpful to get summary statistics per group. We can do this in base R using the aggregate() function.\n\naggregate(length ~ river,\n          data = rivers_r,\n          summary)\n\n    river length.Min. length.1st Qu. length.Median length.Mean length.3rd Qu.\n1   Aripo    17.50000       19.10000      20.10000    20.33077       21.30000\n2 Guanapo    11.20000       17.50000      18.80000    18.29655       19.70000\n  length.Max.\n1    26.40000\n2    23.30000\n\n\n\nThe first argument defines the variable that is being used (length) and grouping (river)\nThe second argument is the data frame that is used\nThe third argument defines the function that is applied across the subsets (in this case that’s the summary() function)\n\nNumbers might not always give you the best insight into your data, so we also visualise our data:\n\nboxplot(length ~ river,\n        data = rivers_r)\n\n\n\n\nWe can use a very similar notation as we did for the summary statistics (length ~ river), so a box plot is created per group.\n\n\n\nrivers_py.describe()\n\n          length\ncount  68.000000\nmean   19.463235\nstd     2.370081\nmin    11.200000\n25%    18.400000\n50%    19.300000\n75%    20.925000\nmax    26.400000\n\n\nThis gives us the standard summary statistics, but in this case we have more than one group (Aripo and Guanapo), so it might be helpful to get summary statistics per group. Here we use the pd.groupby() function to group by river. We only want to have summary statistics for the length variable, so we specify that as well:\n\nrivers_py.groupby(\"river\")[\"length\"].describe()\n\n         count       mean       std   min   25%   50%   75%   max\nriver                                                            \nAripo     39.0  20.330769  1.780620  17.5  19.1  20.1  21.3  26.4\nGuanapo   29.0  18.296552  2.584636  11.2  17.5  18.8  19.7  23.3\n\n\nNumbers might not always give you the best insight into your data, so we also visualise our data:\n\n(\n  ggplot(rivers_py, aes(x = \"river\", y = \"length\"))\n  + geom_boxplot()\n)\n\n\n\n\n\n\n\nThe box plot does appear to suggest that the two samples have different means, and moreover that the guppies in Guanapo may be smaller than the guppies in Aripo. It isn’t immediately obvious that the two populations don’t have equal variances though (box plots are not quite the right tool for this), so we plough on. Who ever said statistics would be glamorous?"
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#assumptions",
    "href": "cs1_practical_two-sample-t-test.html#assumptions",
    "title": "Student’s t-test",
    "section": "Assumptions",
    "text": "Assumptions\nIn order to use a Student’s t-test (and for the results to be strictly valid) we have to make three assumptions:\n\nThe parent distributions from which the samples are taken are both normally distributed (which would lead to the sample data being normally distributed too).\nEach data point in the samples is independent of the others.\nThe parent distributions should have the same variance.\n\nIn this example the first assumption can be ignored as the sample sizes are large enough (because of maths, with Aripo containing 39 and Guanapo 29 samples). If the samples were smaller then we would use the tests from the previous section.\nThe second point we can do nothing about unless we know how the data were collected, so again we ignore it.\nThe third point regarding equality of variance can be tested using either Bartlett’s test (if the samples are normally distributed) or Levene’s test (if the samples are not normally distributed).\nThis is where it gets a bit trickier. Although we don’t care if the samples are normally distributed for the t-test to be valid (because the sample size is big enough to compensate), we do need to know if they are normally distributed in order to decide which variance test to use.\nSo we perform a Shapiro-Wilk test on both samples separately.\n\ntidyverseRPython\n\n\nWe can use the group_by() function to group the data by river, then we perform the Shapiro-Wilk test on the length measurements:\n\n# group data by river and perform test\nrivers %>% \n  group_by(river) %>% \n  shapiro_test(length)\n\n# A tibble: 2 × 4\n  river   variable statistic      p\n  <chr>   <chr>        <dbl>  <dbl>\n1 Aripo   length       0.936 0.0280\n2 Guanapo length       0.949 0.176 \n\n\n\n\nBefore we can do that, we need to convert the data to a format where the data is split by river:\n\n# create a new object (a list) that contains the unstacked data\nuns_rivers <- unstack(rivers_r, form = length ~ river)\n# have a look at the data\nuns_rivers\n\nNow that we’ve separated the data by river we can perform the Shapiro-Wilk test:\n\nshapiro.test(uns_rivers$Aripo)\n\n\n    Shapiro-Wilk normality test\n\ndata:  uns_rivers$Aripo\nW = 0.93596, p-value = 0.02802\n\nshapiro.test(uns_rivers$Guanapo)\n\n\n    Shapiro-Wilk normality test\n\ndata:  uns_rivers$Guanapo\nW = 0.94938, p-value = 0.1764\n\n\n\n\nWe first need to split the data by river.\n\nrivers_py.groupby(\"river\")[\"length\"] \\\n.apply(lambda x: pd.Series(stats.shapiro(x), index=['W-stat','p-value'])) \\\n.reset_index()\n\n     river  level_1    length\n0    Aripo   W-stat  0.935958\n1    Aripo  p-value  0.028023\n2  Guanapo   W-stat  0.949384\n3  Guanapo  p-value  0.176420\n\n\nThe code is a bit convoluted and perhaps there is a more efficient way that I’m not aware of. Anyway, we can do this with the groupby() function from pandas. Next, we only select the length measurements and use the .apply() function to apply the stats.shapiro() test over each group. This returns two values per group: the W-statistic that the Shapiro-Wilk test uses and, the value we’re most interested in, the p-value. Lastly,we use the .reset_index() function to repeat the grouping name.\n\n\n\nWe can see that whilst the Guanapo data is probably normally distributed (p = 0.1764 > 0.05), the Aripo data is unlikely to be normally distributed (p = 0.02802 < 0.05). Remember that the p-value gives the probability of observing each sample if the parent population is actually normally distributed.\nThe Shapiro-Wilk test is quite sensitive to sample size. This means that if you have a large sample then even small deviations from normality will cause the sample to fail the test, whereas smaller samples are allowed to pass with much larger deviations. Here the Aripo data has nearly 40 points in it compared with the Guanapo data and so it is much easier for the Aripo sample to fail compared with the Guanapo data."
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#exercise-qq-rivers",
    "href": "cs1_practical_two-sample-t-test.html#exercise-qq-rivers",
    "title": "Student’s t-test",
    "section": "Exercise: Q-Q plots rivers",
    "text": "Exercise: Q-Q plots rivers\nCreate the Q-Q plots for the two samples and discuss with your neighbour what you see in light of the results from the above Shapiro-Wilk test.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n# we group the data by river\n# then create a panel per river\n# containing the Q-Q plot for that river\nrivers %>% \n  ggplot(aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(river))\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nqqnorm(uns_rivers$Aripo, main = \"Aripo\")\nqqline(uns_rivers$Aripo, col = \"red\")\n\nqqnorm(uns_rivers$Guanapo, main = \"Guanapo\")\nqqline(uns_rivers$Guanapo, col = \"red\")\n\n\n\n\n\n\n\n(\n  ggplot(rivers_py, aes(sample = \"length\"))\n  + stat_qq()\n  + stat_qq_line(colour = \"red\")\n  + facet_wrap(\"river\")\n)\n\n\n\n\n\n\n\nThe Q-Q plots show the opposite of what we found with the Shapiro-Wilk tests: the data for Aripo look pretty normally distributed, whereas the assumption of normality for the Guanapo data is less certain.\nWhat to do? Well, you could be conservative and state that you are not confident that the data in either group are normally distributed. That would be a perfectly reasonable conclusion.\nI would personally not have issues with stating that the Aripo data are probably normally distributed enough."
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#equality-of-variance",
    "href": "cs1_practical_two-sample-t-test.html#equality-of-variance",
    "title": "Student’s t-test",
    "section": "Equality of variance",
    "text": "Equality of variance\n\n\n\n\n\n\nTip\n\n\n\nRemember that statistical tests do not provide answers, they merely suggest patterns. Human interpretation is still a crucial aspect to what we do.\n\n\nThe reason why we’re checking for equality of variance (also referred to as homogeneity of variance) is because many statistical tests assume that the spread of the data within different parental populations (in this case, two) is the same.\nIf that is indeed the case, then the data themselves should have equal spread as well.\nThe Shapiro-Wilk test and the Q-Q plots have shown that some of the data might not be normal enough (although in opposite directions!) and so in order to test for equality of variance we will use Levene’s test.\n\ntidyverseRPython\n\n\nThe function we use is levene_test() from the rstatix library.\nIt takes the data in the form of a formula as follows:\n\nrivers %>% \n  levene_test(length ~ river)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     1    66      1.77 0.188\n\n\nThe key bit of information is the p column. This is the p-value (0.1876) for this test.\n\n\nLevene’s test is not included in the default R packages and may require the installation of an additional package called car (Companion to Applied Regression).\nTo install the car package, run the following command in your console:\n\ninstall.packages(\"car\")\n\nAlternatively, go to Tools > Install packages… > Packages, type in car and press Install\nWe can now perform Levene’s test:\n\nleveneTest(length ~ river, data = rivers)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  1.7732 0.1876\n      66               \n\n\nIgnore any warning you might get about coercion to factors (the test needs to create grouped variables to work and R versions from 4.x onwards do not read in the data as factors).\nThe key bit of information is the 3rd line under the text Pr(>F). This is the p-value for this test.\n\n\nLevene’s test is included in the stats module in scipy. It requires two vectors as input, so we need to subset our data for each river:\n\naripo = rivers_py.query('river == \"Aripo\"')[\"length\"]\nguanapo = rivers_py.query('river == \"Guanapo\"')[\"length\"]\n\nstats.levene(aripo, guanapo)\n\nLeveneResult(statistic=1.7731837331911642, pvalue=0.18756940068805075)\n\n\n\n\n\nThe p-value tells us the probability of observing these two samples if they come from distributions with the same variance. As this probability is greater than our arbitrary significance level of 0.05 then we can be somewhat confident that the necessary assumptions for carrying out Student’s t-test on these two samples was valid. (Once again woohoo!)\n\nBartlett’s test\nIf we had wanted to carry out Bartlett’s test (i.e. if the data had been sufficiently normally distributed) then we would have done:\n\ntidyverseRPython\n\n\nHere we use bartlett.test() from base R. Surprisingly, the rstatix package does not have a built-in equivalent.\nIf we wanted to get the output of the Bartlett test into a tidy format, we could do the following, where we take the rivers data set and pipe it to the bartlett.test() function. Note that we need to define the data using a dot (.), because the first input into bartlett.test() is not the data. We then pipe the output to the tidy() function, which is part of the broom library, which kindly converts the output into a tidy format. Handy!\n\n# load the broom package\nlibrary(broom)\n\n# perform Bartlett's test on the data and tidy\nrivers %>% \n  bartlett.test(length ~ river,\n                data = .) %>% \n  tidy()\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                   \n      <dbl>   <dbl>     <dbl> <chr>                                    \n1      4.47  0.0344         1 Bartlett test of homogeneity of variances\n\n\n\n\n\nbartlett.test(length ~ river, data = rivers_r)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  length by river\nBartlett's K-squared = 4.4734, df = 1, p-value = 0.03443\n\n\nThe relevant p-value is given on the 3rd line.\n\n\nWe’ve already subset our data into guanapo and aripo, vectors that contain our data.\n\nstats.bartlett(aripo, guanapo)\n\nBartlettResult(statistic=4.4734366516240165, pvalue=0.03442568304468286)"
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#implement-and-interpret-the-test",
    "href": "cs1_practical_two-sample-t-test.html#implement-and-interpret-the-test",
    "title": "Student’s t-test",
    "section": "Implement and interpret the test",
    "text": "Implement and interpret the test\nIn this case we’re ignoring the fact that the data are not normal enough, according to the Shapiro-Wilk test. However, this is not entirely naughty, because the sample sizes are pretty large and the t-test is also pretty robust in this case, we can perform a t-test. Remember, this is only allowed because the variances of the two groups (Aripo and Guanapo) are equal.\nPerform a two-sample, two-tailed, t-test:\n\ntidyverseRPython\n\n\n\n# two-sample, two-tailed t-test\nrivers %>% \n  t_test(length ~ river,\n         alternative = \"two.sided\",\n         var.equal = TRUE)\n\n# A tibble: 1 × 8\n  .y.    group1 group2     n1    n2 statistic    df        p\n* <chr>  <chr>  <chr>   <int> <int>     <dbl> <dbl>    <dbl>\n1 length Aripo  Guanapo    39    29      3.84    66 0.000275\n\n\nHere we do the following:\n\nWe take the data set and pipe it to the t_test() function\nThe t_test() function takes the formula in the format variable ~ category\nAgain the alternative is two.sided because we have no prior knowledge about whether the alternative should be greater or less\nThe last argument says whether the variance of the two samples can be assumed to be equal (Student’s t-test) or unequal (Welch’s t-test)\n\nSo, how do we interpret these results?\n\nThe first 5 columns give you information on the variable (.y.), groups and sample size of each group\nThe statistic column gives the t-value of 3.8433 (we need this for reporting)\nThe df column tell us there are 66 degrees of freedom (we need this for reporting)\nThe p column gives us a p-value of 0.0002754\n\n\n\n\nt.test(length ~ river, data = rivers_r,\n       alternative = \"two.sided\",\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  length by river\nt = 3.8433, df = 66, p-value = 0.0002754\nalternative hypothesis: true difference in means between group Aripo and group Guanapo is not equal to 0\n95 percent confidence interval:\n 0.9774482 3.0909868\nsample estimates:\n  mean in group Aripo mean in group Guanapo \n             20.33077              18.29655 \n\n\n\nThe first argument must be in the formula format: variables ~ category\nThe second argument must be the name of the data frame\nThe third argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe fourth argument says whether the variance of the two samples can be assumed to be equal (Student’s t-test) or unequal (Welch’s t-test)\n\nSo, how do we interpret the results?\n\nThe 1st line gives the name of the test and the 2nd line reminds you what the data set was called, and what variables were used.\nThe 3rd line contains the three key outputs from the test:\n\nThe calculated t-value is 3.8433 (we need this for reporting)\nThere are 66 degrees of freedom (we need this for reporting)\nThe p-value is 0.0002754.\n\nThe 4th line simply states the alternative hypothesis in terms of the difference between the two sample means (testing if the two sample means are different is equivalent to testing whether the difference in the means is equal to zero).\nThe 5th and 6th lines give the 95th confidence interval (we don’t need to know this here).\nThe 7th, 8th and 9th lines give the sample means for each group (20.33077 in Aripo and 18.29655 in Guanapo) which we found earlier.\n\n\n\n\nstats.ttest_ind(aripo, guanapo,\n                alternative = \"two-sided\",\n                equal_var = True)\n\nTtest_indResult(statistic=3.8432667461726275, pvalue=0.00027544021976337834)\n\n\n\n\n\nAgain, the p-value is what we’re most interested in. Since the p-value is very small (much smaller than the standard significance level) we choose to say “that it is very unlikely that these two samples came from the same parent distribution and as such we can reject our null hypothesis” and state that:\n\nA Student’s t-test indicated that the mean body length of male guppies in the Guanapo river (18.29 mm) differs significantly from the mean body length of male guppies in the Aripo river (20.33 mm) (t = 3.8433, df = 66, p = 0.0003).\n\n\nNow there’s a conversation starter."
  },
  {
    "objectID": "cs1_practical_two-sample-t-test.html#exercise-turtles",
    "href": "cs1_practical_two-sample-t-test.html#exercise-turtles",
    "title": "Student’s t-test",
    "section": "Exercise: Turtles",
    "text": "Exercise: Turtles\nThis exercise explores serum cholesterol concentrations in turtles.\nUsing the following data, test the null hypothesis that male and female turtles have the same mean serum cholesterol concentrations.\n\n\n\n\n \n  \n    id \n    Male \n    Female \n  \n \n\n  \n    1 \n    220.1 \n    NA \n  \n  \n    2 \n    218.6 \n    NA \n  \n  \n    3 \n    229.6 \n    NA \n  \n  \n    4 \n    228.8 \n    NA \n  \n  \n    5 \n    222.0 \n    NA \n  \n  \n    6 \n    224.1 \n    NA \n  \n  \n    7 \n    226.5 \n    NA \n  \n  \n    8 \n    NA \n    223.4 \n  \n  \n    9 \n    NA \n    221.5 \n  \n  \n    10 \n    NA \n    230.2 \n  \n  \n    11 \n    NA \n    224.3 \n  \n  \n    12 \n    NA \n    223.8 \n  \n  \n    13 \n    NA \n    230.8 \n  \n\n\n\n\n\n\nCreate a tidy data frame and save as a .csv file\nWrite down the null and alternative hypotheses\nImport the data\nSummarise and visualise the data\nCheck your assumptions (normality and variance) using appropriate tests and plots\nPerform a two-sample t-test\nWrite down a sentence that summarises the results that you have found\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nData\nWe’ll stop asking you to manually create your own data files soon, but it’s meant to get you to think about how to record your data. If we’re using a tidy data format, then each variable (thing that you measure) is in its own column. Each observation has its own row.\nThis means that if you would restructure the data from above it would look like this:\n\n\n\n\nturtle\n\n# A tibble: 13 × 2\n   serum sex   \n   <dbl> <chr> \n 1  220. Male  \n 2  219. Male  \n 3  230. Male  \n 4  229. Male  \n 5  222  Male  \n 6  224. Male  \n 7  226. Male  \n 8  223. Female\n 9  222. Female\n10  230. Female\n11  224. Female\n12  224. Female\n13  231. Female\n\n\n\n\nHypotheses\n\\(H_0\\) : male mean \\(=\\) female mean\n\\(H_1\\) : male mean \\(\\neq\\) female mean\n\n\nLoad, summarise and visualise data\nLet’s load the data (I’ve created the .csv file earlier) and explore our data a bit more before we dive into the statistics.\n\ntidyverseRPython\n\n\n\n# load the data\nturtle <- read_csv(\"data/CS1-turtle.csv\")\n\nRows: 13 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (1): serum\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# and have a look\nturtle\n\n# A tibble: 13 × 2\n   serum sex   \n   <dbl> <chr> \n 1  220. Male  \n 2  219. Male  \n 3  230. Male  \n 4  229. Male  \n 5  222  Male  \n 6  224. Male  \n 7  226. Male  \n 8  223. Female\n 9  222. Female\n10  230. Female\n11  224. Female\n12  224. Female\n13  231. Female\n\n\nLet’s summarise the data (although a visualisation is probably much easier to work with):\n\n# create summary statistics for each group\nturtle %>% \n  group_by(sex) %>% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 2 × 11\n  sex    variable     n   min   max median   iqr  mean    sd    se    ci\n  <chr>  <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Female serum        6  222.  231.   224.  5.22  226.  3.87  1.58  4.06\n2 Male   serum        7  219.  230.   224.  6.6   224.  4.26  1.61  3.94\n\n\nand visualise the data:\n\n# visualise the data\nturtle %>% \n  ggplot(aes(x = sex, y = serum)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# load the data\nturtle_r <- read.csv(\"data/CS1-turtle.csv\")\n\n# and have a look\nhead(turtle_r)\n\n  serum  sex\n1 220.1 Male\n2 218.6 Male\n3 229.6 Male\n4 228.8 Male\n5 222.0 Male\n6 224.1 Male\n\n\nand visualise the data:\n\n# visualise the data\nboxplot(serum ~ sex , data = turtle_r)\n\n\n\n\n\n\n\nturtle_py = pd.read_csv(\"data/CS1-turtle.csv\")\n\nturtle_py.describe()\n\n            serum\ncount   13.000000\nmean   224.900000\nstd      3.978274\nmin    218.600000\n25%    222.000000\n50%    224.100000\n75%    228.800000\nmax    230.800000\n\n\nand visualise the data:\n\n(\n  ggplot(turtle_py, aes(x = \"sex\",\n                        y = \"serum\"))\n  + geom_boxplot()\n)\n\n\n\n\n\n\n\nAs always we use the plot and summary to assess three things:\n\nDoes it look like we’ve loaded the data in correctly?\n\nWe have two groups and the extreme values of our plots seem to match with our data set, so I’m happy that we haven’t done anything massively wrong here.\n\nDo we think that there is a difference between the two groups?\n\nWe need the result of the formal test to make sense given the data, so it’s important to develop a sense of what we think is going to happen here. Whilst the ranges of the two groups suggests that the Female serum levels might be higher than the males when we look at things more closely we realise that isn’t the case. The box plot shows that the median values of the two groups is virtually identical and this is backed up by the summary statistics we calculated: the medians are both about 224.1, and the means are fairly close too (225.7 vs 224.2). Based on this, and the fact that there are only 13 observations in total I would be very surprised if any test came back showing that there was a difference between the groups.\n\nWhat do we think about assumptions?\n\nNormality looks a bit worrying: whilst the Male group appears nice and symmetric (and so might be normal), the Female group appears to be quite skewed (since the median is much closer to the bottom than the top). We’ll have to look carefully at the more formal checks to decided whether we think the data are normal enough for us to use a t-test.\nHomogeneity of variance. At this stage the spread of the data within each group looks similar, but because of the potential skew in the Female group we’ll again want to check the assumptions carefully.\n\n\n\n\nAssumptions\nNormality\nLet’s look at the normality of each of the groups separately. There are several ways of getting at the serum values for Male and Female groups separately. All of them come down to splitting the data. Afterwards we use the Shapiro-Wilk (‘formal’ test), followed by Q-Q plots (much more informative).\n\ntidyverseRPython\n\n\n\n# perform Shapiro-Wilk test on each group\nturtle %>% \n  group_by(sex) %>% \n  shapiro_test(serum)\n\n# A tibble: 2 × 4\n  sex    variable statistic     p\n  <chr>  <chr>        <dbl> <dbl>\n1 Female serum        0.842 0.135\n2 Male   serum        0.944 0.674\n\n\n\n\nWe can use the unstack() function to split the data, then access the relevant values.\n\nuns_turtle_r <- unstack(turtle_r, serum ~ sex)\n\nuns_turtle_r\n\n$Female\n[1] 223.4 221.5 230.2 224.3 223.8 230.8\n\n$Male\n[1] 220.1 218.6 229.6 228.8 222.0 224.1 226.5\n\n\nYou can see that the data has been split by sex.\n\nshapiro.test(uns_turtle_r$Male)\n\n\n    Shapiro-Wilk normality test\n\ndata:  uns_turtle_r$Male\nW = 0.94392, p-value = 0.6743\n\nshapiro.test(uns_turtle_r$Female)\n\n\n    Shapiro-Wilk normality test\n\ndata:  uns_turtle_r$Female\nW = 0.84178, p-value = 0.1349\n\n\n\n\n\nturtle_male = turtle_py.query('sex == \"Male\"')[\"serum\"]\nturtle_female = turtle_py.query('sex == \"Female\"')[\"serum\"]\n\n\nstats.shapiro(turtle_male)\n\nShapiroResult(statistic=0.9439237713813782, pvalue=0.6742751598358154)\n\nstats.shapiro(turtle_female)\n\nShapiroResult(statistic=0.8417852520942688, pvalue=0.1348712146282196)\n\n\n\n\n\nThe p-values for both Shapiro-Wilk tests are non-significant which suggests that the data are normal enough. This is a bit surprising given what we saw in the box plot but there are two bits of information that we can use to reassure us.\n\nThe p-value for the Female group is smaller than for the Male group (suggesting that the Female group is closer to being non-normal than the Male group) which makes sense based on our visual observations.\nThe Shapiro-Wilk test is generally quite relaxed about normality for small sample sizes (and notoriously strict for very large sample sizes). For a group with only 6 data points in it, the data would actually have to have a really, really skewed distribution. Given that the Female group only has 6 data points in it, it’s not too surprising that the Shapiro-Wilk test came back saying everything is OK.\n\nGiven these caveats of the Shapiro-Wilk test (I’ll stop mentioning them now, I think I’ve made my opinion clear ;)), let’s look at the Q-Q plots.\n\ntidyverseRPython\n\n\n\n# create Q-Q plots for both groups\nturtle %>% \n  ggplot(aes(sample = serum)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\") +\n  facet_wrap(facets = vars(sex))\n\n\n\n\n\n\n\npar(mfrow=c(1,2))\nqqnorm(uns_turtle_r$Female, main = \"Female\")\nqqline(uns_turtle_r$Female, col = \"red\")\nqqnorm(uns_turtle_r$Male, main = \"Male\")\nqqline(uns_turtle_r$Male, col = \"red\")\n\n\n\n\n\n\n\n# create Q-Q plots for both groups\n(\n  ggplot(turtle_py, aes(sample = \"serum\"))\n  + stat_qq()\n  + stat_qq_line(colour = \"red\")\n  + facet_wrap(\"sex\")\n)\n\n\n\n\n\n\n\nThe results from the Q-Q plots echo what we’ve already seen from the Shapiro-Wilk analyses. The normality of the data in the Male group doesn’t look too bad whereas the those in the Female group looks somewhat dodgy.\nOverall, the assumption of normality of the data doesn’t appear to be very well met at all, but we do have to bear in mind that there are only a few data points in each group and we might just be seeing this pattern in the data due to random chance rather than because the underlying populations are actually not normally distributed. Personally, though I’d edge towards non-normal here.\nHomogeneity of Variance\nIt’s not clear whether the data are normal or not, so it isn’t clear which test to use here. The sensible approach is to do both and hope that they agree (fingers crossed!). Or err on the side of caution and assume they are not normal, but potentially throwing away statistical power (more on that later).\n\ntidyverseRPython\n\n\nBartlett’s test gives us:\n\n# perform Bartlett's test\nbartlett.test(serum ~ sex,\n              data = turtle)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  serum by sex\nBartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n\n\nand Levene’s test gives us:\n\n# perform Levene's test\nturtle %>% \n  levene_test(serum ~ sex)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     1    11     0.243 0.631\n\n\n\n\nBartlett’s test gives us:\n\nbartlett.test(serum ~ sex, turtle_r)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  serum by sex\nBartlett's K-squared = 0.045377, df = 1, p-value = 0.8313\n\n\nand Levene’s test gives us:\n\n# load if needed\n# library(car)\n\nleveneTest(serum ~ sex, turtle_r)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.2434 0.6315\n      11               \n\n\n\n\nBartlett’s test gives us:\n\nstats.bartlett(turtle_male, turtle_female)\n\nBartlettResult(statistic=0.0453770725135282, pvalue=0.8313121829253811)\n\n\nand Levene’s test gives us:\n\nstats.levene(turtle_male, turtle_female)\n\nLeveneResult(statistic=0.24341796609304578, pvalue=0.6314503568954707)\n\n\n\n\n\nThe good news is that both Levene and Bartlett agree that there is homogeneity of variance between the two groups (thank goodness, that’s one less thing to worry about!).\nOverall, what this means is that we’re not too sure about normality, but that homogeneity of variance is pretty good.\n\n\nImplement two-sample t-test\nBecause of the result of the Bartlett test I know that I can carry out a two-sample Student’s t-test. If the variances between the two groups were not equal, then we’d have to perform Welch’s t-test.\n\ntidyverseRPython\n\n\n\n# perform two-sample t-test\nturtle %>% \n  t_test(serum ~ sex,\n         alternative = \"two.sided\",\n         var.equal = TRUE)\n\n# A tibble: 1 × 8\n  .y.   group1 group2    n1    n2 statistic    df     p\n* <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>\n1 serum Female Male       6     7     0.627    11 0.544\n\n\n\n\n\nt.test(serum ~ sex,\n       data = turtle_r,\n       alternative = \"two.sided\",\n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  serum by sex\nt = 0.62681, df = 11, p-value = 0.5436\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -3.575759  6.423378\nsample estimates:\nmean in group Female   mean in group Male \n            225.6667             224.2429 \n\n\n\n\n\nstats.ttest_ind(turtle_male, turtle_female,\n                alternative = \"two-sided\",\n                equal_var = True)\n\nTtest_indResult(statistic=-0.6268108404512706, pvalue=0.543572996867541)\n\n\n\n\n\nWith a p-value of 0.544, this test tells me that there is insufficient evidence to suggest that the means of the two groups are different. A suitable summary sentence would be:\n\nA Student’s two-sample t-test indicated that the mean serum cholesterol level did not differ significantly between Male and Female turtles (t = 0.627, df = 11, p = 0.544).\n\n\n\nDiscussion\nIn reality, because of the ambiguous normality assumption assessment, for this data set I would actually carry out two different tests; the two-sample t-test with equal variance and the Mann-Whitney U test. If both of them agreed then it wouldn’t matter too much which one I reported (I’d personally report both with a short sentence to say that I’m doing that because it wasn’t clear whether the assumption of normality had or had not been met), but it would be acceptable to report just one."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-t-test.html",
    "href": "cs1_practical_two-sample-paired-t-test.html",
    "title": "Paired t-test",
    "section": "",
    "text": "A paired t-test is used when we have two samples of continuous data that can be paired (examples of these sort of data would be weights of individuals before and after a diet). This test is applicable if the number of paired points within the samples is large (>30) or, if the number of points is small, then this test also works when the parent distributions are normally distributed."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-t-test.html#libraries-and-functions",
    "href": "cs1_practical_two-sample-paired-t-test.html#libraries-and-functions",
    "title": "Paired t-test",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(tidyverse)\nA collection of R packages designed for data science\n\n\nlibrary(rstatix)\nConverts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we’ll use.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nrstatix::t_test()\nPerforms a one-sample t-test, Student’s t-test and Welch’s t-test in later sections.\n\n\nrstatix::shapiro_test()\nPerforms a Shapiro-Wilk test for normality.\n\n\nggplot2::stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nggplot2::stat_qq_line()\nAdds a comparison line to the Q-Q plot.\n\n\nggplot2::geom_jitter()\nPlots jittered points by adding a small amount of random variation to each point, to handle overplotting\n\n\ntidyr::pivot_wider()\n“Widens” the data, increasing the number of columns\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nstats::reshape\nReshapes a data frame from ‘long’ to ‘wide’ and vice versa\n\n\nt.test()\nPerforms a one-sample t-test, Student’s t-test and Welch’s t-test in later sections.\n\n\nqqnorm()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nqqline()\nAdds a comparison line to the Q-Q plot.\n\n\nshapiro.test()\nPerforms a Shapiro-Wilk test for normality.\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\nscipy.stats\nA Python module containing statistical functions.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npandas.DataFrame.read_csv\nReads in a .csv file\n\n\npandas.DataFrame.pivot()\nReturn reshaped DataFrame organised by given index / column values.\n\n\nscipy.stats.shapiro()\nPerforms the Shapiro-Wilk test\n\n\nplotnine.stats.stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nplotnine.stats.stat_qq_line()\nAdds a comparison line to the Q-Q plot."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-t-test.html#data-and-hypotheses",
    "href": "cs1_practical_two-sample-paired-t-test.html#data-and-hypotheses",
    "title": "Paired t-test",
    "section": "Data and hypotheses",
    "text": "Data and hypotheses\nFor example, suppose we measure the cortisol levels in 20 adult females (nmol/l) first thing in the morning and again in the evening. We want to test whether the cortisol levels differs between the two measurement times. We will initially form the following null and alternative hypotheses:\n\n\\(H_0\\): There is no difference in cortisol level between times (\\(\\mu M = \\mu E\\))\n\\(H_1\\): There is a difference in cortisol levels between times (\\(\\mu M \\neq \\mu E\\))\n\nWe use a two-sample, two-tailed paired t-test to see if we can reject the null hypothesis.\n\nWe use a two-sample test because we now have two samples\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) means are different from one another rather than that one mean is specifically bigger or smaller than the other\nWe use a paired test because each data point in the first sample can be linked to another data point in the second sample by a connecting factor\nWe’re using a t-test because we’re assuming that the parent populations are normal and have equal variance (We’ll check this in a bit)\n\nThe data are stored in a tidy format in the file data/CS1-twopaired.csv.\n\ntidyverseRPython\n\n\n\n# load the data\ncortisol <- read_csv(\"data/CS1-twopaired.csv\")\n\nRows: 40 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): time\ndbl (2): patient_id, cortisol\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# have a look at the data\ncortisol\n\n# A tibble: 40 × 3\n   patient_id time    cortisol\n        <dbl> <chr>      <dbl>\n 1          1 morning     311.\n 2          2 morning     146.\n 3          3 morning     297 \n 4          4 morning     271.\n 5          5 morning     268.\n 6          6 morning     264.\n 7          7 morning     358.\n 8          8 morning     316.\n 9          9 morning     336.\n10         10 morning     221.\n# … with 30 more rows\n\n\n\n\n\n# load the data\ncortisol_r <- read.csv(\"data/CS1-twopaired.csv\")\n\nhead(cortisol_r)\n\n  patient_id    time cortisol\n1          1 morning    310.6\n2          2 morning    146.1\n3          3 morning    297.0\n4          4 morning    270.9\n5          5 morning    267.5\n6          6 morning    263.8\n\n\n\n\n\n# load the data\ncortisol_py = pd.read_csv('data/CS1-twopaired.csv')\n\n# inspect the data\ncortisol_py.head()\n\n   patient_id     time  cortisol\n0           1  morning     310.6\n1           2  morning     146.1\n2           3  morning     297.0\n3           4  morning     270.9\n4           5  morning     267.5\n\n\n\n\n\nWe can see that the data frame consists of three columns:\n\npatient_id, a unique ID for each patient\ntime when the cortisol level was measured\ncortisol, which contains the measured value.\n\nFor each patient_id there are two measurements: one in the morning and one in the afternoon."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-t-test.html#summarise-and-visualise",
    "href": "cs1_practical_two-sample-paired-t-test.html#summarise-and-visualise",
    "title": "Paired t-test",
    "section": "Summarise and visualise",
    "text": "Summarise and visualise\nIt’s always a good idea to visualise your data, so let’s do that.\n\ntidyverseRPython\n\n\n\n# create a boxplot\ncortisol %>% \n  ggplot(aes(x = time, y = cortisol)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.05) +\n  ylab(\"Cortisol level (nmol/l)\")\n\n\n\n\nHere we use also visualise the actual data points, to get a sense of how these data are spread out. To avoid overlapping the data points (try using geom_point() instead of geom_jitter()), we jitter the data points. What geom_jitter() does is add a small amount of variation to each point.\n\n\n\n# create a boxplot\nboxplot(cortisol ~ time,\n        data = cortisol_r)\n\n\n\n\n\n\n\n(\n  ggplot(cortisol_py,\n    aes(x = \"time\",\n        y = \"cortisol\"))\n  + geom_boxplot()\n  + geom_jitter(width = 0.05)\n  + ylab(\"Cortisol level (nmol/l)\")\n)\n\n\n\n\n\n\n\nHowever, this plot does not capture how the cortisol level of each individual subject has changed though. We can explore the individual changes between morning and evening by looking at the differences between the two times of measurement for each patient.\nTo do this, we need to put our data into a wide format, so we can calculate the change in cortisol level for each patient.\n\ntidyverseRPython\n\n\nIn tidyverse we can use the pivot_wider() function.\n\n# calculate the difference between evening and morning values\ncortisol_diff <- cortisol %>%\n  pivot_wider(names_from = time, values_from = cortisol) %>% \n  mutate(cortisol_change = evening - morning)\n\ncortisol_diff\n\n# A tibble: 20 × 4\n   patient_id morning evening cortisol_change\n        <dbl>   <dbl>   <dbl>           <dbl>\n 1          1    311.   273.            -37.4\n 2          2    146.    65.7           -80.4\n 3          3    297    257.            -40.4\n 4          4    271.   321              50.1\n 5          5    268.    80.3          -187. \n 6          6    264.   379.            116. \n 7          7    358.   163.           -195. \n 8          8    316.   294.            -22  \n 9          9    336.   140.           -196. \n10         10    221.   231.             10.4\n11         11    366    131.           -235. \n12         12    256.   114.           -142. \n13         13    432.   217.           -215. \n14         14    208.    60.1          -148. \n15         15    324.   199.           -125. \n16         16    388.   170.           -218. \n17         17    332    160.           -172. \n18         18    414.   179.           -235. \n19         19    405.   286            -119. \n20         20    356.   226.           -130. \n\n\nAfter this we can plot our data:\n\n# plot the data\n  ggplot(cortisol_diff, aes(y = cortisol_change)) +\n  geom_boxplot() +\n  ylab(\"Change in cortisol (nmol/l)\")\n\n\n\n\nThe differences in cortisol levels appear to be very much less than zero, meaning that the evening cortisol levels appear to be much lower than the morning ones. As such we would expect that the test would give a pretty significant result.\nAn alternative representation would be to plot the data points for both evening and morning and connect them by patient:\n\n# plot cortisol levels by patient\ncortisol %>% \n  ggplot(aes(x = time,\n             y = cortisol,\n             group = patient_id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\nThis gives a similar picture to what the boxplot was telling us, that for most patients the cortisol levels are higher in the morning than in the evening.\n\n\n\ncortisol_diff_r <- reshape(cortisol_r,\n        idvar = \"patient_id\",\n        timevar = \"time\",\n        direction = \"wide\")\n\n# add new column with difference\ncortisol_diff_r$cortisol_change <- cortisol_diff_r$cortisol.evening - cortisol_diff_r$cortisol.morning\n\nhead(cortisol_diff_r)\n\n  patient_id cortisol.morning cortisol.evening cortisol_change\n1          1            310.6            273.2           -37.4\n2          2            146.1             65.7           -80.4\n3          3            297.0            256.6           -40.4\n4          4            270.9            321.0            50.1\n5          5            267.5             80.3          -187.2\n6          6            263.8            379.3           115.5\n\n\nAfter this we can plot our data:\n\nboxplot(cortisol_diff_r$cortisol_change)\n\n\n\n\nThe differences in cortisol levels appear to be very much less than zero, meaning that the evening cortisol levels appear to be much lower than the morning ones. As such we would expect that the test would give a pretty significant result.\nAn alternative representation would be to plot the data points for both evening and morning and connect them by patient:\n\nmatplot(t(cortisol_diff_r[ , 2:3]),\n        pch = 1,\n        type = c(\"b\"),\n        col = 1:20)\n\n\n\n\nAs far as I am aware of, there isn’t a straightforward method of plotting paired data using the base R functionality. Hence the data gymnastics:\n\nthe default plot() function doesn’t support this - the standard matplot() function does\nthe t function transposes the data, and I’m only selecting the second and third columns ([ , 2:3]) which contain the paired morning/evening measurements.\nto group (pair) the data, we’re using colours, one for each of the 20 patients (col = 1:20)\n\nThis gives a similar picture to what the boxplot was telling us, that for most patients the cortisol levels are higher in the morning than in the evening.\n\n\n\n# reformat the data into a 'wide' format\ncortisol_diff_py = pd.pivot(cortisol_py, index = \"patient_id\", columns = \"time\", values = \"cortisol\")\n\n# add a new column with difference between\n# evening and morning cortisol levels\ncortisol_diff_py[\"cortisol_change\"] = cortisol_diff_py[\"evening\"].subtract(cortisol_diff_py[\"morning\"])\n      \n# have a look at the format\ncortisol_diff_py.head()\n\ntime        evening  morning  cortisol_change\npatient_id                                   \n1             273.2    310.6            -37.4\n2              65.7    146.1            -80.4\n3             256.6    297.0            -40.4\n4             321.0    270.9             50.1\n5              80.3    267.5           -187.2\n\n\nAfter this we can plot our data:\n\n# plot the data\n(\n  ggplot(cortisol_diff_py, aes(x = \"1\", y = \"cortisol_change\"))\n  + geom_boxplot()\n  + ylab(\"Change in cortisol (nmol/l)\")\n)\n\n\n\n\nThe differences in cortisol levels appear to be very much less than zero, meaning that the evening cortisol levels appear to be much lower than the morning ones. As such we would expect that the test would give a pretty significant result.\nAn alternative representation would be to plot the data points for both evening and morning and connect them by patient:\n\n# plot cortisol levels by patient\n(\n  ggplot(cortisol_py,\n    aes(x = \"time\",\n        y = \"cortisol\",\n        group = \"patient_id\"))\n  + geom_point()\n  + geom_line()\n)\n\n\n\n\nThis gives a similar picture to what the boxplot was telling us, that for most patients the cortisol levels are higher in the morning than in the evening."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-t-test.html#assumptions",
    "href": "cs1_practical_two-sample-paired-t-test.html#assumptions",
    "title": "Paired t-test",
    "section": "Assumptions",
    "text": "Assumptions\nYou will do this in the exercise!"
  },
  {
    "objectID": "cs1_practical_two-sample-paired-t-test.html#implement-and-interpret-the-test",
    "href": "cs1_practical_two-sample-paired-t-test.html#implement-and-interpret-the-test",
    "title": "Paired t-test",
    "section": "Implement and interpret the test",
    "text": "Implement and interpret the test\nPerform a two-sample, two-tailed, paired t-test:\n\ntidyverseRPython\n\n\n\n# perform the test\ncortisol %>% \n  t_test(cortisol ~ time,\n         alternative = \"two.sided\",\n         paired = TRUE)\n\n\nThe first argument gives the formula\nThe second argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe third argument says that the data are paired\n\n\n\n\nt.test(cortisol ~ time,\n       alternative = \"two.sided\",\n       paired = TRUE,\n       data = cortisol_r)\n\n\n    Paired t-test\n\ndata:  cortisol by time\nt = -5.1833, df = 19, p-value = 5.288e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -162.96038  -69.20962\nsample estimates:\nmean difference \n       -116.085 \n\n\n\nThe first two arguments define the formula\nThe third argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\nThe fourth argument says that the data are paired\n\nFrom our perspective the value of interested is in the p column (p-value = 5.29 \\(\\times\\) 10-5). Given that this is substantially less than 0.05 we can reject the null hypothesis and state:\n\n\nPython has a dedicated function for testing the null hypothesis that two related or repeated samples have identical average (expected) values: stats.ttest_rel().\n\nstats.ttest_rel(cortisol_diff_py[\"evening\"],\ncortisol_diff_py[\"morning\"],\nalternative = \"two-sided\")\n\nTtest_relResult(statistic=-5.183290089830781, pvalue=5.288037286955356e-05)\n\n\nSince the p-value = 5.29 \\(\\times\\) 10-5) and thus substantially less than 0.05 we can reject the null hypothesis and state:\n\n\n\n\nA two-tailed, paired t-test indicated that the average cortisol level in adult females differed significantly between the morning (313.5 nmol/l) and the evening (197.4 nmol/l) (t = -5.2, df = 19, p = 5.3 * 10-5)."
  },
  {
    "objectID": "cs1_practical_two-sample-paired-t-test.html#exercise-assumptions",
    "href": "cs1_practical_two-sample-paired-t-test.html#exercise-assumptions",
    "title": "Paired t-test",
    "section": "Exercise: Assumptions",
    "text": "Exercise: Assumptions\nCheck the assumptions necessary for this this paired t-test. Was a paired t-test an appropriate test?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nA paired test is really just a one-sample test in disguise. We actually don’t care too much about the distributions of the individual groups. Instead we care about the properties of the differences. So for a paired t-test to be valid for this data set, we need the differences between the morning and evening values to be normally distributed.\nLet’s check this with the Shapiro-Wilk test and Q-Q plots, using the wide data frames we created earlier.\n\ntidyversebase RPython\n\n\nPerform Shapiro-Wilk test:\n\n# perform Shapiro-Wilk test on cortisol differences\ncortisol_diff %>% \n  shapiro_test(cortisol_change)\n\n# A tibble: 1 × 3\n  variable        statistic     p\n  <chr>               <dbl> <dbl>\n1 cortisol_change     0.924 0.116\n\n\nCreate Q-Q plot:\n\n# create the Q-Q plot\ncortisol_diff %>% \n  ggplot(aes(sample = cortisol_change)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")\n\n\n\n\n\n\nPerform Shapiro-Wilk test:\n\n# perform Shapiro-Wilk test on cortisol differences\nshapiro.test(cortisol_diff_r$cortisol_change)\n\n\n    Shapiro-Wilk normality test\n\ndata:  cortisol_diff_r$cortisol_change\nW = 0.92362, p-value = 0.1164\n\n\nCreate Q-Q plot:\n\nqqnorm(cortisol_diff_r$cortisol_change)\nqqline(cortisol_diff_r$cortisol_change, col = \"red\")\n\n\n\n\n\n\nPerform Shapiro-Wilk test:\n\n# perform Shapiro-Wilk test on cortisol differences\nstats.shapiro(cortisol_diff_py[\"cortisol_change\"])\n\nShapiroResult(statistic=0.9236220121383667, pvalue=0.11635485291481018)\n\n\nCreate Q-Q plot:\n\n# create the Q-Q plot\n(\n  ggplot(cortisol_diff_py,\n    aes(sample = \"cortisol_change\"))\n    + stat_qq()\n    + stat_qq_line(colour = \"red\")\n)\n\n\n\n\n\n\n\nThe Shapiro-Wilk test says that the data are normal enough and whilst the Q-Q plot is mostly fine, there is some suggestion of snaking at the bottom left. I’m actually OK with this because the suggestion of snaking is actually only due to a single point (the last point on the left). If you cover that point up with your thumb (or finger of your choice) then the remaining points in the Q-Q plot look pretty darn good, and so the suggestion of snaking is actually driven by only a single point (which can happen by chance). As such I’m happy that the assumption of normality is well-met in this case. This single point check is a useful thing to remember when assessing diagnostic plots.\nSo, yep, a paired t-test is appropriate for this data set."
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html",
    "href": "cs1_practical_one-sample-wilcoxon.html",
    "title": "Wilcoxon signed-rank test",
    "section": "",
    "text": "This test also considers a single sample, however for this test (in contrast to the one sample t-test) we don’t have to assume that the parent distribution is normally distributed. We do still need the parent distribution (and consequently the sample) to be the same shape and scale. In this test we look to see if the median of the parent distributions differs significantly from a given hypothesised value (in contrast with the t-test that looks at the mean)."
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html#libraries-and-functions",
    "href": "cs1_practical_one-sample-wilcoxon.html#libraries-and-functions",
    "title": "Wilcoxon signed-rank test",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(tidyverse)\nA collection of R packages designed for data science\n\n\nlibrary(rstatix)\nConverts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we’ll use.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nrstatix::wilcox_test()\nPerforms one and two sample Wilcoxon tests.\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nwilcoxon.test()\nPerforms one- and two-sample Wilcoxon tests on vectors of data; the latter is also known as ‘Mann-Whitney’ test.\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\nscipy.stats\nA Python module containing statistical functions.\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nscipy.stats.wilcoxon()\nCalculate the Wilcoxon signed-rank test."
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html#data-and-hypotheses",
    "href": "cs1_practical_one-sample-wilcoxon.html#data-and-hypotheses",
    "title": "Wilcoxon signed-rank test",
    "section": "Data and hypotheses",
    "text": "Data and hypotheses\nAgain, we use the fishlength data set. The one-sample Wilcoxon signed-rank test allows to see if the median body length is different from a specified value. Here we want to test whether the data support the hypothesis that the median body is actually 20 mm. The following null and alternative hypotheses are very similar to those used for the one sample t-test:\n\n\\(H_0\\): The median body length is equal to 20 mm (\\(\\mu =\\) 20).\n\\(H_1\\): The median body length is not equal to 20 mm (\\(\\mu \\neq\\) 20).\n\nWe will use a one-sample, two-tailed Wilcoxon signed-rank test to see if we should reject the null hypothesis or not."
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html#summarise-and-visualise",
    "href": "cs1_practical_one-sample-wilcoxon.html#summarise-and-visualise",
    "title": "Wilcoxon signed-rank test",
    "section": "Summarise and visualise",
    "text": "Summarise and visualise\nWe did this before in the previous section, nothing really should have changed between now and then (if it has then you’re not off to a good start on this practical!)"
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html#assumptions",
    "href": "cs1_practical_one-sample-wilcoxon.html#assumptions",
    "title": "Wilcoxon signed-rank test",
    "section": "Assumptions",
    "text": "Assumptions\nIn order to use a one-sample Wilcoxon rank-sum test for this analysis (and for the results to be strictly valid) we have to make two assumptions:\n\nThe data are distributed symmetrically around the median\nEach data point in the sample is independent of the others. This is the same as for the t-test and is a common feature of nearly all statistical tests. Lack of independence in your data is really tough to deal with (if not impossible) and a large part of proper experimental design is ensuring this.\n\nWhilst there are formal statistical tests for symmetry we will opt for a simple visual inspection using both a box plot and a histogram.\nPlot a histogram and a box plot of the data:\n\ntidyverseRPython\n\n\nLet’s first determine the median, so we can use that to compare our data to.\n\n# determine the median\nmedian_fishlength <- fishlengthDF %>% \n  summarise(median_fishlength = median(length)) %>% \n  pull(median_fishlength)\n\n\n# create a histogram\nfishlengthDF %>% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 10) +\n  geom_vline(xintercept = median_fishlength,\n             colour = \"red\")\n\n\n\n# create box plot\nfishlengthDF %>% \n  ggplot(aes(y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\nLet’s first determine the median, so we can use that to compare our data to.\n\n# determine the median\nmedian_fishlength <- median(fishlength_r)\n\n\nhist(fishlength_r, breaks = 10)\nabline(v = median_fishlength,\n       col = \"red\")\n\nboxplot(fishlength_r)\n\nYou get the following plots:\n\n\n\n\n\n\n\nLet’s first determine the median, so we can use that to compare our data to.\n\nmedian_fishlength = fishlength_py.length.median()\n\n\n# create a histogram\n(\nggplot(fishlength_py, aes(x = \"length\"))\n+ geom_histogram(bins = 10)\n+ geom_vline(xintercept = median_fishlength,\n             colour = \"red\")\n)\n\n\n\n(\n# create box plot\nggplot(fishlength_py,\naes(x = 1,\n    y = 'length'))\n+ geom_boxplot()\n)\n\n\n\n\n\n\n\nHere we can see that whilst the distribution isn’t perfectly symmetric, neither is it heavily skewed to the left or right and we can make the call that the distribution is symmetric enough for us to be happy with the results of the test."
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html#implement-and-interpret-the-test",
    "href": "cs1_practical_one-sample-wilcoxon.html#implement-and-interpret-the-test",
    "title": "Wilcoxon signed-rank test",
    "section": "Implement and interpret the test",
    "text": "Implement and interpret the test\nPerform a one-sample, two-tailed Wilcoxon signed-rank test:\n\ntidyverseRPython\n\n\n\nfishlengthDF %>% \n  wilcox_test(length ~ 1,\n              mu = 20,\n              alternative = \"two.sided\")\n\n# A tibble: 1 × 6\n  .y.    group1 group2         n statistic       p\n* <chr>  <chr>  <chr>      <int>     <dbl>   <dbl>\n1 length 1      null model    29      67.5 0.00122\n\n\nThe syntax is identical to the one-sample t-test we carried out earlier.\n\nthe formula, here we give it length ~ 1 to indicate it is a one-sample test on length\nthe mu is the median to be tested under the null hypothesis, here it is 20\nthe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less. We have no prior assumptions on whether the alternative median fish length would be greater or less than 20, so we choose two.sided.\n\n\nthe statistic column gives us the t-statistic of 67.5 (we’ll need this for reporting)\nthe n column gives us the sample size of 29\nthe p column gives us the p-value of 0.0012\n\n\n\n\nwilcox.test(fishlength_r, \n            mu = 20,\n            alternative = \"two.sided\")\n\nWarning in wilcox.test.default(fishlength_r, mu = 20, alternative =\n\"two.sided\"): cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  fishlength_r\nV = 67.5, p-value = 0.001222\nalternative hypothesis: true location is not equal to 20\n\n\nThe syntax is identical to the one-sample t-test we carried out earlier.\n\nThe first argument must be a numerical vector of data values.\nThe second argument must be a number and is the median to be tested under the null hypothesis.\nThe third argument gives the type of alternative hypothesis and must be one of two.sided, greater or less.\nThe first two lines give a warning (not an error) message regarding the implementation of this test. This can be safely ignored in this case as the p-value is so small, but essentially, it’s letting you know that some of the data values are identical to each other. This is not supposed to happen as we should be dealing with continuous data for this test, but in practice it’s not something that we need to worry ourselves with.\nThe 3rd line gives the name of the test and the 4th line reminds you what the data set was called\nThe 5th line contains the two key outputs from the test:\n\nThe calculated statistic is 67.5 (we’ll need this for reporting)\nThe p-value is 0.001222.\n\nThe 6th line simply states the alternative hypothesis\n\n\n\n\nstats.wilcoxon(fishlength_py.length - 20,\n               alternative = \"two-sided\")\n\nWilcoxonResult(statistic=67.5, pvalue=0.0011760820729428206)\n\n\nThe syntax is similar to what we did earlier:\n\nThe 1st argument we give to the wilcoxon() function is an array of the differences between our data points and the median to be tested under the null hypothesis, i.e. our data points (fishlength_py.length) minus the test median (20, in this case).\nThe 2nd argument gives us the type of alternative hypothesis and must be one of “two-sided”, “larger”, or “smaller”. This is in contrast to the 1-sample t-test in Python that can only mange two-sided alternative hypotheses.\n\n\n\n\nAgain, the p-value is what we’re most interested in. It gives the probability of us getting a sample such as ours if the null hypothesis were actually true. So, in this case since our p-value is less than 0.05 we can reject our null hypothesis and state that:\n\nA one-sample Wilcoxon signed-rank test indicated that the median body length of male guppies (\\(\\mu\\) = 18.8 mm) differs significantly from 20 mm (V = 67.5, n = 29, p = 0.0012).\n\n\n\nThe above sentence is an adequate concluding statement for this test and is what we would write in any paper or report. Note that we have included (in brackets) information on the median value of the group (\\(\\mu\\) = 18.8 mm), the test statistic (V = 67.5), the number of observations (n = 29), and the p-value (p = 0.0012)."
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html#exercise-gastric-juices-revisited",
    "href": "cs1_practical_one-sample-wilcoxon.html#exercise-gastric-juices-revisited",
    "title": "Wilcoxon signed-rank test",
    "section": "Exercise: Gastric juices (revisited)",
    "text": "Exercise: Gastric juices (revisited)\nPerforming a Wilcoxon signed-rank test:\n\nAnalyse the drug data set from before using a one-sample Wilcoxon signed-rank test\nDiscuss with a (virtual) neighbour which of the two tests you feel is best suited to the data.\nDoes it matter in this case?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : median \\(=\\) 45s\n\\(H_1\\) : median \\(\\neq\\) 45s\n\n\nAssumptions\nFrom the box plot from the previous exercise we already know that the data are symmetric enough for the test to be valid.\n\n\nWilcoxon signed-rank test\nPerforming the test:\n\ntidyverseRPython\n\n\n\ndissolving %>% \n  wilcox_test(dissolving_time ~ 1,\n              mu = 45,\n              alternative = \"two.sided\")\n\n# A tibble: 1 × 6\n  .y.             group1 group2         n statistic     p\n* <chr>           <chr>  <chr>      <int>     <dbl> <dbl>\n1 dissolving_time 1      null model     8        22 0.641\n\n\n\n\n\nwilcox.test(dissolving_r$dissolving_time,\n            mu = 45,\n            alternative = \"two.sided\")\n\n\n    Wilcoxon signed rank exact test\n\ndata:  dissolving_r$dissolving_time\nV = 22, p-value = 0.6406\nalternative hypothesis: true location is not equal to 45\n\n\n\n\n\nstats.wilcoxon(dissolving_py.dissolving_time - 45,\n               alternative = \"two-sided\")\n\nWilcoxonResult(statistic=14.0, pvalue=0.640625)\n\n\n\n\n\n\nA one-sample Wilcoxon-signed rank test indicated that the median dissolving time of the drug is not significantly different from 45 s (V=22, n=8 , p=0.64)\n\n\n\n\n\nDiscussion\nIn terms of choosing between the two test we can see that both meet their respective assumptions and so both tests are valid. In this case both tests also agree in terms of their conclusions i.e. that the average dissolving time (either mean or median) doesn’t differ significantly from the proposed value of 45 s.\n\nSo one answer would be that it doesn’t matter which test you use.\nAnother answer would be that you should pick the test that measures the quantity you’re interested in i.e. if you care about medians then use the Wilcoxon test, whereas if you care about means then use the t-test.\nA final answer would be that, since both test are valid we would prefer to use the test with greater power. t-tests always have more power than Wilcoxon tests (as long as they’re valid) and so we could report that one. (We’ll talk about this in the last session but power is effectively the capacity of a test to detect a significant difference - so more power is better)."
  },
  {
    "objectID": "cs1_practical_one-sample-wilcoxon.html#key-points",
    "href": "cs1_practical_one-sample-wilcoxon.html#key-points",
    "title": "Wilcoxon signed-rank test",
    "section": "Key points",
    "text": "Key points\n\n\n\n\n\n\nNote\n\n\n\n\nOne-sample tests are used when you have a single sample of continuous data\nThe t-test assumes that the data are normally distributed and independent of each other\nThe Wilcoxon signed-rank test does not assume a normal distribution, but does require independent samples and symmetry around the median\nA good way of assessing the assumption of normality is by checking the data against a Q-Q plot"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Core statistics",
    "section": "",
    "text": "Authors and contributors:\nMartin van Rongen, Matt Castle, Rob Nicholls, Holly Pavey, Vicki Hodgson\nWelcome to Core statistics!\nThese sessions are intended to enable you to perform core data analysis techniques appropriately and confidently using R.\nThey are not a “how to mindlessly use a stats program” course!"
  },
  {
    "objectID": "index.html#core-aims",
    "href": "index.html#core-aims",
    "title": "Welcome to Core statistics",
    "section": "Core aims",
    "text": "Core aims\nThere are several things that we try to achieve during this course.\n\n\n\n\n\n\nCourse aims\n\n\n\nTo know what to do when presented with an arbitrary data set e.g.\n\nKnow what data analysis techniques are available\nKnow which ones are allowable\nBe able to carry these out and understand the results"
  },
  {
    "objectID": "index.html#core-topics",
    "href": "index.html#core-topics",
    "title": "Welcome to Core statistics",
    "section": "Core topics",
    "text": "Core topics\n\nSimple hypothesis testing\nCategorical predictors\nContinuous predictors\nTwo predictors\nMultiple predictors\nPower analysis"
  },
  {
    "objectID": "index.html#practicals",
    "href": "index.html#practicals",
    "title": "Welcome to Core statistics",
    "section": "Practicals",
    "text": "Practicals\nEach practical document is divided up into various sections. In each section there will be some explanatory text which should help you to understand what is going on and what you’re trying to achieve. There may be a list of commands relevant to that section which will be displayed in boxes like this:\n\n\n\n\n\n\nConditional operators\n\n\n\nTo set filtering conditions, use the following relational operators:\n\n> is greater than\n>= is greater than or equal to\n< is less than\n<= is less than or equal to\n== is equal to\n!= is different from\n%in% is contained in\n\nTo combine conditions, use the following logical operators:\n\n& AND\n| OR"
  },
  {
    "objectID": "index.html#index-datasets",
    "href": "index.html#index-datasets",
    "title": "Welcome to Core statistics",
    "section": "Datasets",
    "text": "Datasets\nThis course uses various data sets. The easiest way of accessing these is by creating an R-project in RStudio. Then download the data folder here by right-clicking on the link and Save as…. Next unzip the file and copy it into your working directory. Your data should then be accessible via <working-directory-name>/data/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "More info later."
  },
  {
    "objectID": "cs1_practical_two-sample-mann-whitney.html",
    "href": "cs1_practical_two-sample-mann-whitney.html",
    "title": "Mann-Whitney U test",
    "section": "",
    "text": "This test also compares two samples, however for this test (in contrast to Student’s t-test) we don’t have to assume that the parent distributions are normally distributed. In order to compare the medians of the two groups we do still need the parent distributions (and consequently the samples) to both have the same shape and variance. In this test we look to see if the medians of the two parent distributions differ significantly from each other."
  },
  {
    "objectID": "cs1_practical_two-sample-mann-whitney.html#libraries-and-functions",
    "href": "cs1_practical_two-sample-mann-whitney.html#libraries-and-functions",
    "title": "Mann-Whitney U test",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(tidyverse)\nA collection of R packages designed for data science\n\n\nlibrary(rstatix)\nConverts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we’ll use.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nrstatix::wilcox_test()\nPerforms one- and two-sample Wilcoxon tests on vectors of data; the latter is also known as ‘Mann-Whitney’ test\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nwilcox.test()\nPerforms one- and two-sample Wilcoxon tests on vectors of data; the latter is also known as ‘Mann-Whitney’ test\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\nscipy.stats\nA Python module containing statistical functions.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npandas.DataFrame.pivot()\nReturn reshaped DataFrame organised by given index / column values.\n\n\nscipy.stats.mannwhitneyu()\nCalculate the Mann-Whitney U test"
  },
  {
    "objectID": "cs1_practical_two-sample-mann-whitney.html#data-and-hypotheses",
    "href": "cs1_practical_two-sample-mann-whitney.html#data-and-hypotheses",
    "title": "Mann-Whitney U test",
    "section": "Data and hypotheses",
    "text": "Data and hypotheses\nAgain, we use the rivers data set. We want to test whether the median body length of male guppies differs between samples. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The difference in median body length between the two groups is 0 \\((\\mu A - \\mu G = 0)\\)\n\\(H_1\\): The difference in median body length between the two groups is not 0 \\((\\mu A - \\mu G \\neq 0)\\)\n\nWe use a two-tailed Mann-Whitney U test to see if we can reject the null hypothesis."
  },
  {
    "objectID": "cs1_practical_two-sample-mann-whitney.html#summarise-and-visualise",
    "href": "cs1_practical_two-sample-mann-whitney.html#summarise-and-visualise",
    "title": "Mann-Whitney U test",
    "section": "Summarise and visualise",
    "text": "Summarise and visualise\nWe did this in the previous section."
  },
  {
    "objectID": "cs1_practical_two-sample-mann-whitney.html#assumptions",
    "href": "cs1_practical_two-sample-mann-whitney.html#assumptions",
    "title": "Mann-Whitney U test",
    "section": "Assumptions",
    "text": "Assumptions\nWe have checked these previously."
  },
  {
    "objectID": "cs1_practical_two-sample-mann-whitney.html#implement-and-interpret-the-test",
    "href": "cs1_practical_two-sample-mann-whitney.html#implement-and-interpret-the-test",
    "title": "Mann-Whitney U test",
    "section": "Implement and interpret the test",
    "text": "Implement and interpret the test\nPerform a two-tailed, Mann-Whitney U test:\n\ntidyverseRPython\n\n\n\nrivers %>% \n  wilcox_test(length ~ river,\n              alternative = \"two.sided\")\n\n# A tibble: 1 × 7\n  .y.    group1 group2     n1    n2 statistic        p\n* <chr>  <chr>  <chr>   <int> <int>     <dbl>    <dbl>\n1 length Aripo  Guanapo    39    29       841 0.000646\n\n\n\nThe first argument must be in the formula format: variable ~ category\nThe second argument gives the type of alternative hypothesis and must be one of two.sided, greater or less\n\nYou may get a warning message in the console stating cannot compute exact p-value with ties. This just means that some of the data points have exactly the same value which affects the internal mathematics slightly. However, given that the p-value is so very small, this is not something that we need to worry about.\n\nThe first 5 columns give you information on the variable (.y.), groups and sample size of each group\nThe statistic column gives the t-value of 841 (we need this for reporting)\nThe p column gives us a p-value of 0.0006464.\n\n\n\n\nwilcox.test(length ~ river, data = rivers_r,\n            alternative = \"two.sided\")\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  length by river\nW = 841, p-value = 0.0006464\nalternative hypothesis: true location shift is not equal to 0\n\n\nYou may get a warning message in the console stating cannot compute exact p-value with ties. This just means that some of the data points have exactly the same value which affects the internal mathematics slightly. However, given that the p-value is so very small, this is not something that we need to worry about.\nAfter the warning message:\n\nThe 1st line gives the name of the test and the 2nd line reminds you what the dataset was called, and what variables were used\nThe 3rd line contains the two key outputs from the test:\n\nThe calculated W-value is 841 (we’ll use this in reporting)\nThe p-value is 0.0006464.\n\nThe 4th line simply states the alternative hypothesis in terms of the difference between the two sample medians in that if there were a difference then one distribution would be shifted relative to the other.\n\n\n\nBefore we can implement the Mann-Whitney U test, we need to reformat our data a bit.\nThe stats.mannwhitneyu() function requires the numerical input for the two groups it needs to compare.\nThe easiest way is to reformat our data from the long format where all the data are stacked on top of one another to the wide format, where the length values are in separate columns for the two rivers.\nWe can do this with the pd.pivot() function. We save the output in a new object and then access the values as required. It keeps all the data separate, meaning that there will be missing values NaN in this format. The stats.mannwhitneyu() function doesn’t ignore missing values by default and we can specify this in the nan_policy, by setting this argument to omit.\n\n# reformat the data into a 'wide' format\nrivers_py_wide = pd.pivot(rivers_py,\n                          columns = 'river',\n                          values = 'length')\n      \n# have a look at the format\nrivers_py_wide.head()\n\nriver  Aripo  Guanapo\n0        NaN     19.1\n1        NaN     23.3\n2        NaN     18.2\n3        NaN     16.4\n4        NaN     19.7\n\n\n\n# perform the Mann-Whitney U test\n# ignoring the missing values\nstats.mannwhitneyu(rivers_py_wide['Aripo'],\n                   rivers_py_wide['Guanapo'],\n                   nan_policy = 'omit')\n\nMannwhitneyuResult(statistic=841.0, pvalue=0.0006463668392349246)\n\n\n\n\n\nGiven that the p-value is less than 0.05 we can reject the null hypothesis at this confidence level. Again, the p-value on the 3rd line is what we’re most interested in. Since the p-value is very small (much smaller than the standard significance level) we choose to say “that it is very unlikely that these two samples came from the same parent distribution and as such we can reject our null hypothesis”.\nTo put it more completely, we can state that:\n\nA Mann-Whitney test indicated that the median body length of male guppies in the Guanapo river (18.8 mm) differs significantly from the median body length of male guppies in the Aripo river (20.1 mm) (W = 841, p = 0.0006)."
  },
  {
    "objectID": "cs1_practical_two-sample-mann-whitney.html#exercise-turtles-revisited",
    "href": "cs1_practical_two-sample-mann-whitney.html#exercise-turtles-revisited",
    "title": "Mann-Whitney U test",
    "section": "Exercise: Turtles (revisited)",
    "text": "Exercise: Turtles (revisited)\nAnalyse the turtle data set from before using a Mann-Whitney U test.\nWe follow the same process as with Student’s t-test.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : male median \\(=\\) female median\n\\(H_1\\) : male median \\(\\neq\\) female median\n\n\nSummarise and visualise\nThis is the same as before.\n\n\nAssumptions\nWe’ve already checked that the variances of the two groups are similar, so we’re OK there. Whilst the Mann-Whitney U test doesn’t require normality or symmetry of distributions it does require that the distributions have the same shape. In this example, with just a handful of data points in each group, it’s quite hard to make this call one way or another. My advice in this case would be say that unless it’s obvious that the distributions are very different we can just allow this assumption to pass, and you’re only going see obvious differences in distribution shape when you have considerably more data points than we have here.\n\n\nCarry out a Mann-Whitney test\n\ntidyverseRPython\n\n\n\nturtle %>% \n  wilcox_test(serum ~ sex,\n              alternative = \"two.sided\")\n\n# A tibble: 1 × 7\n  .y.   group1 group2    n1    n2 statistic     p\n* <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>\n1 serum Female Male       6     7        26 0.534\n\n\n\n\n\nwilcox.test(serum ~ sex,\n            data = turtle_r,\n            alternative = \"two.sided\")\n\n\n    Wilcoxon rank sum exact test\n\ndata:  serum by sex\nW = 26, p-value = 0.5338\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n# reformat the data into a 'wide' format\nturtle_py_wide = pd.pivot(turtle_py,\n                          columns = 'sex',\n                          values = 'serum')\n      \n# have a look at the format\nturtle_py_wide.head()\n\nsex  Female   Male\n0       NaN  220.1\n1       NaN  218.6\n2       NaN  229.6\n3       NaN  228.8\n4       NaN  222.0\n\n\n\n# perform the Mann-Whitney U test\n# ignoring the missing values\nstats.mannwhitneyu(turtle_py_wide['Male'],\n                   turtle_py_wide['Female'],\n                   nan_policy = 'omit')\n\nMannwhitneyuResult(statistic=16.0, pvalue=0.5337995337995338)\n\n\n\n\n\nThis gives us exactly the same conclusion that we got from the two-sample t-test i.e. that there isn’t any significant difference between the two groups.\n\nA Mann-Whitney test indicated that there wasn’t a significant difference in the median Serum Cholesterol levels between male and female turtles (W = 26, p = 0.534)"
  },
  {
    "objectID": "cs1_practical_one-sample-t-test.html#libraries-and-functions",
    "href": "cs1_practical_one-sample-t-test.html#libraries-and-functions",
    "title": "One-sample t-test",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(tidyverse)\nA collection of R packages designed for data science\n\n\nlibrary(rstatix)\nConverts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we’ll use.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nrstatix::t_test()\nPerforms a one-sample t-test, Student’s t-test and Welch’s t-test in later sections.\n\n\nrstatix::shapiro_test()\nPerforms a Shapiro-Wilk test for normality.\n\n\nggplot2::stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nggplot2::stat_qq_line()\nAdds a comparison line to the Q-Q plot.\n\n\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nt.test()\nPerforms a one-sample t-test, Student’s t-test and Welch’s t-test in later sections.\n\n\nqqnorm()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nqqline()\nAdds a comparison line to the Q-Q plot.\n\n\nshapiro.test()\nPerforms a Shapiro-Wilk test for normality.\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\nscipy.stats\nA Python module containing statistical functions.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\nscipy.stats.shapiro()\nPerform the Shapiro-Wilk test for normality.\n\n\nscipy.stats.ttest_1samp()\nCalculate the T-test for the mean of ONE group of scores.\n\n\nplotnine.stats.stat_qq()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nplotnine.stats.stat_qq_line()\nAdds a comparison line to the Q-Q plot."
  },
  {
    "objectID": "cs1_practical_one-sample-t-test.html#data-and-hypotheses",
    "href": "cs1_practical_one-sample-t-test.html#data-and-hypotheses",
    "title": "One-sample t-test",
    "section": "Data and hypotheses",
    "text": "Data and hypotheses\nFor example, suppose we measure the body lengths of male guppies (in mm) collected from the Guanapo River in Trinidad. We want to test whether the data support the hypothesis that the mean body is actually 20 mm. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean body length is equal to 20mm (\\(\\mu =\\) 20).\n\\(H_1\\): The mean body length is not equal to 20mm (\\(\\mu \\neq\\) 20).\n\nWe will use a one-sample, two-tailed t-test to see if we should reject the null hypothesis or not.\n\nWe use a one-sample test because we only have one sample.\nWe use a two-tailed t-test because we want to know if our data suggest that the true (population) mean is different from 20 mm in either direction rather than just to see if it is greater than or less than 20 mm (in which case we would use a one-tailed test).\nWe’re using a t-test because we don’t know any better yet and because I’m telling you to. We’ll look at what the precise assumptions/requirements need to be in a moment.\n\nMake sure you have downloaded the data (see: Datasets) and placed it within your working directory.\n\ntidyverseRPython\n\n\nFirst we load the relevant libraries:\n\n# load tidyverse\nlibrary(tidyverse)\n\n# load rstatix, a tidyverse-friendly stats package\nlibrary(rstatix)\n\nWe then read in the data and create a table containing the data.\n\n# import the data\nfishlengthDF <- read_csv(\"data/CS1-onesample.csv\")\n\nfishlengthDF\n\n# A tibble: 29 × 3\n      id river   length\n   <dbl> <chr>    <dbl>\n 1     1 Guanapo   19.1\n 2     2 Guanapo   23.3\n 3     3 Guanapo   18.2\n 4     4 Guanapo   16.4\n 5     5 Guanapo   19.7\n 6     6 Guanapo   16.6\n 7     7 Guanapo   17.5\n 8     8 Guanapo   19.9\n 9     9 Guanapo   19.1\n10    10 Guanapo   18.8\n# … with 19 more rows\n\n\nThe first line reads the data into R and creates an object called a tibble, which is a type of data frame. This data frame contains 3 columns: a unique id, river encoding the river and length with the measured guppy length.\n\n\nWe then read in the data and create a vector containing the data.\n\n# import the data\nfishlengthDF <- read.csv(\"data/CS1-onesample.csv\")\n\n# create a vector containing the data\nfishlength_r <- fishlengthDF$length\n\nThe first line reads the data into R and creates an object called a data frame. This data frame only contains a single column of numbers called “Guanapo” (the name of the river). In most situations, and for most statistical analyses, having our data stored in a data frame is exactly what we’d want. However, for one sample tests we actually need our data to be stored as a vector. So, the second line extracts the values that are in the Guanapo column of our fishlengthDF data frame and creates a simple vector of numbers that we have called fishlength_r This step is only necessary for one-sample tests and when we look at more complex data sets, we won’t need to do this second step at all.\n\n\nWe then read the data in:\n\n# load the data\nfishlength_py = pd.read_csv('data/CS1-onesample.csv')\n\n# inspect the data\nfishlength_py.head()\n\n   id    river  length\n0   1  Guanapo    19.1\n1   2  Guanapo    23.3\n2   3  Guanapo    18.2\n3   4  Guanapo    16.4\n4   5  Guanapo    19.7"
  },
  {
    "objectID": "cs1_practical_one-sample-t-test.html#summarise-and-visualise",
    "href": "cs1_practical_one-sample-t-test.html#summarise-and-visualise",
    "title": "One-sample t-test",
    "section": "Summarise and visualise",
    "text": "Summarise and visualise\nSummarise the data and visualise it:\n\ntidyverseRPython\n\n\n\nsummary(fishlengthDF)\n\n       id        river               length    \n Min.   : 1   Length:29          Min.   :11.2  \n 1st Qu.: 8   Class :character   1st Qu.:17.5  \n Median :15   Mode  :character   Median :18.8  \n Mean   :15                      Mean   :18.3  \n 3rd Qu.:22                      3rd Qu.:19.7  \n Max.   :29                      Max.   :23.3  \n\nfishlengthDF %>% \n  ggplot(aes(x = river, y = length)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nsummary(fishlength_r)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   11.2    17.5    18.8    18.3    19.7    23.3 \n\nboxplot(fishlength_r, main = \"Male guppies\", ylab = \"Length (mm)\")\n\n\n\n\n\n\nFirst we have a look at a numerical summary of the data:\n\nfishlength_py.describe()\n\n              id     length\ncount  29.000000  29.000000\nmean   15.000000  18.296552\nstd     8.514693   2.584636\nmin     1.000000  11.200000\n25%     8.000000  17.500000\n50%    15.000000  18.800000\n75%    22.000000  19.700000\nmax    29.000000  23.300000\n\n\n\n(\n  ggplot(fishlength_py,\n    aes(x = 'river',\n        y = 'length'))\n  + geom_boxplot()\n)\n\n\n\n\n\n\n\nThe data do not appear to contain any obvious errors, and whilst both the mean and median are less than 20 (18.3 and 18.8 respectively) it is not absolutely certain that the sample mean is sufficiently different from this value to be “statistically significant”, although we may anticipate such a result."
  },
  {
    "objectID": "cs1_practical_one-sample-t-test.html#assumptions",
    "href": "cs1_practical_one-sample-t-test.html#assumptions",
    "title": "One-sample t-test",
    "section": "Assumptions",
    "text": "Assumptions\nWhen it comes to one-sample tests, we have two options:\n\nt-test\nWilcoxon signed-rank test\n\nFor us to use a t-test for this analysis (and for the results to be valid) we have to make two assumptions:\n\nThe parent distribution from which the sample is taken is normally distributed (and as such the sample data are normally distributed themselves).\n\n\n\n\n\n\n\nNote\n\n\n\nIt is worth noting though that the t-test is actually pretty robust in situations where the sample data are not normal. For sufficiently large sample sizes (your guess is as good as mine, but conventionally this means about 30 data points), you can use a t-test without worrying about whether the underlying population is normally distributed or not.\n\n\n\nEach data point in the sample is independent of the others. This is in general not something that can be tested for and instead has to be considered from the sampling procedure. For example, taking repeated measurements from the same individual would generate data that are not independent.\n\nThe second point we know nothing about and so we ignore it here (this is an issue that needs to be considered from the experimental design), whereas the first assumption can be checked. There are three ways of checking for normality:\nIn increasing order of rigour, we have\n\nHistogram\nQuantile-quantile plot\nShapiro-Wilk test\n\n\nHistogram of the data\nPlot a histogram of the data, which gives:\n\ntidyverseRPython\n\n\n\nfishlengthDF %>% \n  ggplot(aes(x = length)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\nhist(fishlength_r, breaks = 15)\n\n\n\n\n\n\n\n(\n  ggplot(fishlength_py, aes(x = \"length\"))\n  + geom_histogram(bins = 15)\n)\n\n\n\n\n\n\n\nThe distribution appears to be uni-modal and symmetric, and so it isn’t obviously non-normal. However, there are a lot of distributions that have these simple properties but which aren’t normal, so this isn’t exactly rigorous. Thankfully there are other, more rigorous tests.\nNB. By even looking at this distribution to assess the assumption of normality we are already going far beyond what anyone else ever does. Nevertheless, we will continue.\n\n\nQ-Q plot of the data\nQ-Q plot is the short for quantile-quantile plot. This diagnostic plot (as it is sometimes called) is a way of comparing two distributions. How Q-Q plots work won’t be explained here but ask a demonstrator if you really want to know what is going on.\nConstruct a Q-Q Plot of the quantiles of the data against the quantiles of a normal distribution:\n\ntidyverseRPython\n\n\n\nfishlengthDF %>% \n  ggplot(aes(sample = length)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\n# plot the Q-Q plot\nqqnorm(fishlength_r)\n\n# and add a comparison line\nqqline(fishlength_r)\n\n\n\n\n\n\n\n(\n  ggplot(fishlength_py, aes(sample = \"length\"))\n  + stat_qq()\n  + stat_qq_line()\n)\n\n\n\n\n\n\n\nWhat is important to know is that if the data were normally distributed then all of the points should lie on (or close to) the diagonal line in this graph.\nIn this case, the points lie quite close to the line for the most part but the sample quantiles (points) from either end of the sample distribution are either smaller (below the line on the left) or larger (above the line on the right) than expected if they were supposed to be normally distributed. This suggests that the sample distribution is a bit more spread out than would be expected if it came from a normal distribution.\nIt is important to recognise that there isn’t a simple unambiguous answer when interpreting these types of graph, in terms of whether the assumption of normality has been well met or not and instead it often boils down to a matter of experience.\nIt is a very rare situation indeed where the assumptions necessary for a test will be met unequivocally and a certain degree of personal interpretation is always needed. Here you have to ask yourself whether the data are normal “enough” for you to be confident in the validity of the test.\nBelow are four examples of QQ plots for different types of distributions:\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a normal distribution. Even here you can see that the points do not all lie perfectly on the diagonal line in the QQ plot, and a certain amount of deviation at the top and bottom of the graph can happen just by chance (if I were to draw a different set of point then the graph would look slightly different).\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a uniform distribution. Uniform distributions are more condensed than normal distributions, and this is reflected in the QQ plot having a very pronounced S-shaped pattern to it (this is colloquially known as snaking).\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from a t distribution. t distributions are more spread out than normal distributions, and this is reflected in the QQ plot again having a very pronounced S-shaped pattern to it, but this time the snaking is a reflection of that observed for the uniform distribution.\n\n\n\n\n\nThese two graphs relate to 200 data points that have been drawn from an exponential distribution. Exponential distributions are not symmetric and are very skewed compared with normal distributions. The significant right-skew in this distribution is reflected in the QQ plot again having points that curve away above the diagonal line at both ends (a left-skew would have the points being below the line at both ends).\nIn all four cases it is worth noting that the deviations are only at the ends of the plot.\n\n\nShapiro-Wilk test\nThis is one of a number of formal statistical test that assess whether a given sample of numbers come from a normal distribution. It calculates the probability of getting the sample data if the underlying distribution is in fact normal. It is very easy to carry out in R.\nPerform a Shapiro-Wilk test on the data:\n\ntidyverseRPython\n\n\n\nfishlengthDF %>% \n  shapiro_test(length)\n\n# A tibble: 1 × 3\n  variable statistic     p\n  <chr>        <dbl> <dbl>\n1 length       0.949 0.176\n\n\n\nvariable indicated the variable that was used to perform the test on\nstatistic gives the calculated W-value (0.9493842)\np gives the calculated p-value (0.1764229)\n\n\n\n\nshapiro.test(fishlength_r)\n\n\n    Shapiro-Wilk normality test\n\ndata:  fishlength_r\nW = 0.94938, p-value = 0.1764\n\n\n\nThe 1st line gives the name of the test and the 2nd line reminds you what the data set was called\nThe 3rd line contains the two key outputs from the test:\nThe calculated w-value is 0.9494 (we don’t need to know this)\nThe p-value is 0.1764\n\n\n\n\nstats.shapiro(fishlength_py.length)\n\nShapiroResult(statistic=0.9493839740753174, pvalue=0.17642046511173248)\n\n\n\n\n\nAs the p-value is bigger than 0.05 (say) then we can say that there is insufficient evidence to reject the null hypothesis that the sample came from a normal distribution.\nIt is important to recognise that the Shapiro-Wilk test is not without limitations. It is rather sensitive to the sample size being considered. In general, for small sample sizes, the test is very relaxed about normality (and nearly all data sets are considered normal), whereas for large sample sizes the test can be overly strict, and it can fail to recognise data sets that are very nearly normal indeed.\n\n\nAssumptions overview\n\n\n\n\n\n\nImportant\n\n\n\nIn terms of assessing the assumptions of a test it is always worth considering several methods, both graphical and analytic, and not just relying on a single method.\n\n\nIn the fishlength example, the graphical Q-Q plot analysis was not especially conclusive as there was some suggestion of snaking in the plots, but the Shapiro-Wilk test gave a non-significant p-value (0.1764). Putting these two together, along with the original histogram and the recognition that there were only 30 data points in the data set I personally would be happy that the assumptions of the t-test were met well enough to trust the result of the t-test, but you may not be…\nIn which case we would consider an alternative test that has less stringent assumptions (but is less powerful): the one-sample Wilcoxon signed-rank test."
  },
  {
    "objectID": "cs1_practical_one-sample-t-test.html#implement-the-test",
    "href": "cs1_practical_one-sample-t-test.html#implement-the-test",
    "title": "One-sample t-test",
    "section": "Implement the test",
    "text": "Implement the test\nPerform a one-sample, two-tailed t-test:\n\ntidyverseRPython\n\n\n\nfishlengthDF %>% \n  t_test(length ~ 1,\n         mu = 20,\n         alternative = \"two.sided\")\n\nThe t_test() function requires three arguments:\n\nthe formula, here we give it length ~ 1 to indicate it is a one-sample test on length\nthe mu is the mean to be tested under the null hypothesis, here it is 20\nthe alternative argument gives the type of alternative hypothesis and must be one of two.sided, greater or less. We have no prior assumptions on whether the alternative fish length would be greater or less than 20, so we choose two.sided.\n\n\n\n\nt.test(fishlength_r,\n       mu = 20,\n       alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  fishlength_r\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n\n\n\nThe first argument must be a numerical vector of data values.\nThe second argument must be a number and is the mean to be tested under the null hypothesis.\nThe third argument gives the type of alternative hypothesis and must be one of two.sided, greater or less. We have no prior assumptions on whether the alternative fish length would be greater or less than 20, so we choose two.sided.\n\n\n\n\nstats.ttest_1samp(fishlength_py.length,\n                  popmean = 20, \n                  alternative = \"two-sided\")\n\n\nThe first argument must be a numerical series of data values.\nThe second argument must be a number and is the mean to be tested under the null hypothesis.\n\nIn Python you can only two a two-sided 1-sample t-test (i.e. you can only test whether the mean is different from 20 but not whether it is greater than or less than – why they chose to do this is beyond me)."
  },
  {
    "objectID": "cs1_practical_one-sample-t-test.html#interpreting-the-output-and-report-results",
    "href": "cs1_practical_one-sample-t-test.html#interpreting-the-output-and-report-results",
    "title": "One-sample t-test",
    "section": "Interpreting the output and report results",
    "text": "Interpreting the output and report results\nThis is the output that you should now see in the console window:\n\ntidyverseRPython\n\n\n\n\n# A tibble: 1 × 7\n  .y.    group1 group2         n statistic    df       p\n* <chr>  <chr>  <chr>      <int>     <dbl> <dbl>   <dbl>\n1 length 1      null model    29     -3.55    28 0.00139\n\n\n\nthe statistic column gives us the t-statistic of -3.5492 (we’ll need this for reporting)\nthe df column tells us there are 28 degrees of freedom (again we’ll need this for reporting)\nthe p column gives us the p-value of 0.00139\n\n\n\n\n\n\n    One Sample t-test\n\ndata:  fishlength_r\nt = -3.5492, df = 28, p-value = 0.001387\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.31341 19.27969\nsample estimates:\nmean of x \n 18.29655 \n\n\n\nThe 1st line gives the name of the test and the 2nd line reminds you what the dataset was called\nThe 3rd line contains the three key outputs from the test:\n\nThe calculated t-value is -3.5492 (we’ll need this for reporting)\nThere are 28 degrees of freedom (again we’ll need this for reporting)\nThe p-value is 0.001387.\n\nThe 4th line simply states the alternative hypothesis\nThe 5th and 6th lines give the 95th confidence interval (we don’t need to know this)\nThe 7th, 8th and 9th lines give the sample mean again (18.29655).\n\n\n\n\n\nTtest_1sampResult(statistic=-3.5491839564647205, pvalue=0.0013868577835348002)\n\n\nThe output is very minimal. The 1st number in brackets is the t-value and the 2nd number is the p-value\n\n\n\nThe p-value is what we’re mostly interested in. It gives the probability of us getting a sample such as ours if the null hypothesis were actually true.\nSo:\n\na high p-value means that there is a high probability of observing a sample such as ours and the null hypothesis is probably true whereas\na low p-value means that there is a low probability of observing a sample such as ours and the null hypothesis is probably not true.\n\nIt is important to realise that the p-value is just an indication and there is no absolute certainty here in this interpretation.\nPeople, however like more definite answers and so we pick an artificial probability threshold (called a significance level) in order to be able to say something more decisive. The standard significance level is 0.05 and since our p-value is smaller than this we choose to say that “it is very unlikely that we would have this particular sample if the null hypothesis were true”. Consequently, we can reject our null hypothesis and state that:\n\nA one-sample t-test indicated that the mean body length of male guppies (\\(\\mu\\) = 18.29mm) differs significantly from 20 mm (t = -3.55, df = 28, p = 0.0014).\n\nThe above sentence is an adequate concluding statement for this test and is what we would write in any paper or report. Note that we have included (in brackets) information on the actual mean value of our group(\\(\\mu\\) = 18.29mm), the test statistic (t = -3.55), the degrees of freedom (df = 28), and the p-value (p = 0.0014). In some journals you are only required to report whether the p-value is less than the critical value (e.g. p < 0.05) but I would always recommend reporting the actual p-value obtained.\nPlease feel free to ask a demonstrator if any aspect of this section is unclear as this does form the core of classical hypothesis testing and the logic here applies to all of the rest of the tests."
  },
  {
    "objectID": "cs1_practical_one-sample-t-test.html#exercise-gastric-juices",
    "href": "cs1_practical_one-sample-t-test.html#exercise-gastric-juices",
    "title": "One-sample t-test",
    "section": "Exercise: gastric juices",
    "text": "Exercise: gastric juices\nThe following data are the dissolving times (in seconds) of a drug in agitated gastric juice:\n42.7, 43.4, 44.6, 45.1, 45.6, 45.9, 46.8, 47.6\nDo these results provide any evidence to suggest that dissolving time for this drug is different from 45 seconds?\n\nCreate a tidy data frame and save it in .csv format\nWrite down the null and alternative hypotheses.\nSummarise and visualise the data and perform an appropriate one-sample t-test.\n\nWhat can you say about the dissolving time? (what sentence would you use to report this)\n\nCheck the assumptions for the test.\n\nWas the test valid?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nHypotheses\n\\(H_0\\) : mean \\(=\\) 45s\n\\(H_1\\) : mean \\(\\neq\\) 45s\n\n\nData, summarise & visualise\nWe can create a data frame in Excel and save it as an .csv file, for example as CS1-gastric_juices.csv. It contains two columns, an id column and a dissolving_time column with the measured values.\n\ntidyverseRPython\n\n\n\n# load the data\ndissolving <- read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving\n\n# A tibble: 8 × 2\n     id dissolving_time\n  <dbl>           <dbl>\n1     1            42.7\n2     2            43.4\n3     3            44.6\n4     4            45.1\n5     5            45.6\n6     6            45.9\n7     7            46.8\n8     8            47.6\n\n# summarise the data\nsummary(dissolving)\n\n       id       dissolving_time\n Min.   :1.00   Min.   :42.70  \n 1st Qu.:2.75   1st Qu.:44.30  \n Median :4.50   Median :45.35  \n Mean   :4.50   Mean   :45.21  \n 3rd Qu.:6.25   3rd Qu.:46.12  \n Max.   :8.00   Max.   :47.60  \n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\ndissolving %>% \n  ggplot(aes(x = dissolving_time)) +\n  geom_histogram(bins = 4)\n\n\n\n# create a boxplot\ndissolving %>% \n  ggplot(aes(y = dissolving_time)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# load the data\ndissolving_r <- read.csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving_r\n\n  id dissolving_time\n1  1            42.7\n2  2            43.4\n3  3            44.6\n4  4            45.1\n5  5            45.6\n6  6            45.9\n7  7            46.8\n8  8            47.6\n\n# summarise the data\nsummary(dissolving_r)\n\n       id       dissolving_time\n Min.   :1.00   Min.   :42.70  \n 1st Qu.:2.75   1st Qu.:44.30  \n Median :4.50   Median :45.35  \n Mean   :4.50   Mean   :45.21  \n 3rd Qu.:6.25   3rd Qu.:46.12  \n Max.   :8.00   Max.   :47.60  \n\n\n\nhist(dissolving_r$dissolving_time)\n\n\n\nboxplot(dissolving_r$dissolving_time)\n\n\n\n\n\n\n\n# load the data\ndissolving_py = pd.read_csv(\"data/CS1-gastric_juices.csv\")\n\n# have a look at the data\ndissolving_py.head()\n\n# summarise the data\n\n   id  dissolving_time\n0   1             42.7\n1   2             43.4\n2   3             44.6\n3   4             45.1\n4   5             45.6\n\ndissolving_py.describe()\n\n            id  dissolving_time\ncount  8.00000         8.000000\nmean   4.50000        45.212500\nstd    2.44949         1.640068\nmin    1.00000        42.700000\n25%    2.75000        44.300000\n50%    4.50000        45.350000\n75%    6.25000        46.125000\nmax    8.00000        47.600000\n\n\nWe can look at the histogram and box plot of the data:\n\n# create a histogram\n(\n  ggplot(dissolving_py,\n    aes(x = \"dissolving_time\"))\n  + geom_histogram(bins = 4)\n)\n\n\n\n\n\n# create a box plot\n(\n  ggplot(dissolving_py,\n    aes(x = 1, y = \"dissolving_time\"))\n  + geom_boxplot()\n)\n\n\n\n\nPython (or plotnine in particular) gets a bit cranky if you try to create a geom_boxplot but do not define the x aesthetic. Hence us putting it as 1. The value is meaningless, however.\n\n\n\nThere are only 8 data points, so the histogram is rather uninformative. Thankfully the box plot is a bit more useful here. We can see:\n\nThere don’t appear to be any major errors in data entry and there aren’t any huge outliers\nThe median value in the box-plot (the thick black line) is pretty close to 45 and so I wouldn’t be surprised if the mean of the data isn’t significantly different from 45. We can confirm that by looking at the mean and median values that we calculated using the summary command from earlier.\nThe data appear to be symmetric, and so whilst we can’t tell if they’re normal they’re a least not massively skewed.\n\n\n\nAssumptions\nNormality:\n\ntidyverseRPython\n\n\n\n# perform Shapiro-Wilk test\ndissolving %>% \n  shapiro_test(dissolving_time)\n\n# A tibble: 1 × 3\n  variable        statistic     p\n  <chr>               <dbl> <dbl>\n1 dissolving_time     0.980 0.964\n\n\n\n# create a Q-Q plot\ndissolving %>% \n  ggplot(aes(sample = dissolving_time)) +\n  stat_qq() +\n  stat_qq_line(colour = \"red\")\n\n\n\n\n\n\n\nshapiro.test(dissolving_r$dissolving_time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dissolving_r$dissolving_time\nW = 0.98023, p-value = 0.9641\n\n\n\nqqnorm(dissolving_r$dissolving_time)\nqqline(dissolving_r$dissolving_time)\n\n\n\n\n\n\n\n# Perform Shapiro-Wilk test to check normality\nstats.shapiro(dissolving_py.dissolving_time)\n\nShapiroResult(statistic=0.9802345037460327, pvalue=0.9640554785728455)\n\n\n\n# Create a Q-Q plot\n(\n  ggplot(dissolving_py,\n    aes(sample = \"dissolving_time\"))\n  + stat_qq()\n  + stat_qq_line()\n)\n\n\n\n\n\n\n\n\nThe Shapiro test has a p-value of 0.964 which (given that it is bigger than 0.05) suggests that the data are normal enough.\nThe Q-Q plot isn’t perfect, with some deviation of the points away from the line but since the points aren’t accelerating away from the line and, since we only have 8 points, we can claim, with some slight reservations, that the assumption of normality appears to be adequately well met.\n\nOverall, we are somewhat confident that the assumption of normality is well-enough met for the t-test to be an appropriate method for analysing the data. Note the ridiculous number of caveats here and the slightly political/slippery language I’m using. This is intentional and reflects the ambiguous nature of assumption checking. This is an important approach to doing statistics that you need to embrace.\nIn reality, if I found myself in this situation I would also try doing a non-parametric test on the data (Wilcoxon signed-rank test) and see whether I get the same conclusion about whether the median dissolving time differs from 45s. Technically, you don’t know about the Wilcoxon test yet as you haven’t done that section of the materials. Anyway, if I get the same conclusion then my confidence in the result of the test goes up considerably; it doesn’t matter how well an assumption has been met, I get the same result. If on the other hand I get a completely different conclusion from carrying out the non-parametric test then all bets are off; I now have very little confidence in my test result as I don’t know which one to believe (in the case that the assumptions of the test are a bit unclear). In this example a Wilcoxon test also gives us a non-significant result and so all is good.\n\n\nImplement test\n\ntidyverseRPython\n\n\n\n# perform one-sample t-test\ndissolving %>% \n  t_test(dissolving_time ~ 1,\n         mu = 45,\n         alternative = \"two.sided\")\n\n# A tibble: 1 × 7\n  .y.             group1 group2         n statistic    df     p\n* <chr>           <chr>  <chr>      <int>     <dbl> <dbl> <dbl>\n1 dissolving_time 1      null model     8     0.366     7 0.725\n\n\n\n\n\nt.test(dissolving_r$dissolving_time,\n       mu = 45 ,\n       alternative = \"two.sided\")\n\n\n    One Sample t-test\n\ndata:  dissolving_r$dissolving_time\nt = 0.36647, df = 7, p-value = 0.7248\nalternative hypothesis: true mean is not equal to 45\n95 percent confidence interval:\n 43.84137 46.58363\nsample estimates:\nmean of x \n  45.2125 \n\n\n\n\n\nstats.ttest_1samp(dissolving_py.dissolving_time,\n                  popmean = 45, \n                  alternative = \"two-sided\")\n\nTtest_1sampResult(statistic=0.36647318560088843, pvalue=0.7248382429835611)\n\n\n\n\n\n\nA one-sample t-test indicated that the mean dissolving time of the drug is not significantly different from 45s (t=0.366 , df=7 , p=0.725)\n\n\n\nAnd that, is that."
  },
  {
    "objectID": "cs2_practical_anova.html#purpose-and-aim",
    "href": "cs2_practical_anova.html#purpose-and-aim",
    "title": "ANOVA",
    "section": "Purpose and aim",
    "text": "Purpose and aim\nAnalysis of variance or ANOVA is a test than can be used when we have multiple samples of continuous response data. Whilst it is possible to use ANOVA with only two samples, it is generally used when we have three or more groups. It is used to find out if the samples came from parent distributions with the same mean. It can be thought of as a generalisation of the two-sample Student’s t-test."
  },
  {
    "objectID": "cs2_practical_anova.html#libraries-and-functions",
    "href": "cs2_practical_anova.html#libraries-and-functions",
    "title": "ANOVA",
    "section": "Libraries and functions",
    "text": "Libraries and functions\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\n\ntidyverseRPython\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nlibrary(tidyverse)\nA collection of R packages designed for data science\n\n\nlibrary(rstatix)\nConverts base R stats functions to a tidyverse-friendly format. Also contains extra functionality that we’ll use.\n\n\nlibrary(ggResidpanel)\nCreates diagnostic plots using ggplot2\n\n\n\n\n\n\nLibrary\nFunction\nDescription\n\n\n\n\nrstatix\nget_summary_stats()\nComputes summary statistics\n\n\nggResidpanel\nresid_panel()\nCreates diagnostic plots\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlm()\nFits a linear model\n\n\nanova()\nCarries out an ANOVA on a linear model\n\n\nqqnorm()\nPlots a Q-Q plot for comparison with a normal distribution.\n\n\nqqline()\nAdds a comparison line to the Q-Q plot.\n\n\nshapiro.test()\nPerforms a Shapiro-Wilk test for normality.\n\n\n\n\n\n\n\n\n\n\n\n\nLibraries\nDescription\n\n\n\n\nplotnine\nThe Python equivalent of ggplot2.\n\n\npandas\nA Python data analysis and manipulation tool.\n\n\nscipy.stats\nA Python module containing statistical functions.\n\n\nstatsmodels\nA Python module for statistical models, conducting tests and statistical data exploration.\n\n\n\n\n\n\n\n\n\n\nFunctions\nDescription\n\n\n\n\npandas.DataFrame.read_csv\nReads in a .csv file\n\n\npandas.DataFrame.head()\nPlots the first few rows\n\n\npandas.DataFrame.describe()\nGives summary statistics\n\n\npandas.DataFrame.groupby()\nGroup DataFrame using a mapper or by a Series of columns\n\n\npandas.DataFrame.query()\nQuery the columns of a DataFrame with a boolean expression\n\n\nscipy.stats.levene()\nPerforms Levene’s test for equality of variance"
  },
  {
    "objectID": "cs2_practical_anova.html#data-and-hypotheses",
    "href": "cs2_practical_anova.html#data-and-hypotheses",
    "title": "ANOVA",
    "section": "Data and hypotheses",
    "text": "Data and hypotheses\nFor example, suppose we measure the feeding rate of oyster catchers (shellfish per hour) at three sites characterised by their degree of shelter from the wind, imaginatively called exposed (E), partially sheltered (P) and sheltered (S). We want to test whether the data support the hypothesis that feeding rates don’t differ between locations. We form the following null and alternative hypotheses:\n\n\\(H_0\\): The mean feeding rates at all three sites is the same \\(\\mu E = \\mu P = \\mu S\\)\n\\(H_1\\): The mean feeding rates are not all equal.\n\nWe will use a one-way ANOVA test to check this.\n\nWe use a one-way ANOVA test because we only have one predictor variable (the categorical variable location).\nWe’re using ANOVA because we have more than two groups and we don’t know any better yet with respect to the exact assumptions.\n\nThe data are stored in the file data/CS2-oystercatcher.csv."
  },
  {
    "objectID": "cs2_practical_anova.html#summarise-and-visualise",
    "href": "cs2_practical_anova.html#summarise-and-visualise",
    "title": "ANOVA",
    "section": "Summarise and visualise",
    "text": "Summarise and visualise\n\ntidyverseRPython\n\n\nFirst we read in the data.\n\n# load data\noystercatcher <- read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher\n\n# A tibble: 120 × 2\n   site    feeding\n   <chr>     <dbl>\n 1 exposed    12.2\n 2 exposed    13.1\n 3 exposed    17.9\n 4 exposed    13.9\n 5 exposed    14.1\n 6 exposed    18.4\n 7 exposed    15.0\n 8 exposed    10.3\n 9 exposed    11.8\n10 exposed    12.5\n# … with 110 more rows\n\n\nThe oystercatcher data set contains two columns:\n\na site column with information on the amount of shelter of the feeding location\na feeding column containing feeding rates\n\nNext, we get some basic descriptive statistics:\n\n# get some basic descriptive statistics\noystercatcher %>% \n  group_by(site) %>% \n  get_summary_stats(type = \"common\")\n\n# A tibble: 3 × 11\n  site      variable     n   min   max median   iqr  mean    sd    se    ci\n  <chr>     <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 exposed   feeding     40  8.35  18.6   13.9  3.40  13.8  2.44 0.386 0.781\n2 partial   feeding     40 10.8   23.0   16.9  2.82  17.1  2.62 0.414 0.838\n3 sheltered feeding     40 18.9   28.5   23.2  3.79  23.4  2.42 0.383 0.774\n\n\nFinally, we plot the data by site:\n\n# plot the data\noystercatcher %>% \n  ggplot(aes(x = site, y = feeding)) +\n  geom_boxplot()\n\n\n\n\n\n\nFirst we read in the data.\n\n# load data\noystercatcher_r <- read.csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# have a look\nhead(oystercatcher_r)\n\n     site  feeding\n1 exposed 12.17551\n2 exposed 13.07392\n3 exposed 17.93969\n4 exposed 13.89178\n5 exposed 14.05166\n6 exposed 18.36498\n\n\nThe oystercatcher data set contains two columns:\n\na site column with information on the amount of shelter of the feeding location\na feeding column containing feeding rates\n\nNext, we get some basic descriptive statistics. We have three groups, so to get the summary statistics by group we do the following:\n\naggregate(feeding ~ site,\n          data = oystercatcher_r,\n          summary)\n\n       site feeding.Min. feeding.1st Qu. feeding.Median feeding.Mean\n1   exposed     8.350801       12.184961      13.946420    13.822899\n2   partial    10.795969       15.601927      16.927683    17.081666\n3 sheltered    18.856999       21.403028      23.166246    23.355503\n  feeding.3rd Qu. feeding.Max.\n1       15.581748    18.560404\n2       18.416708    23.021250\n3       25.197096    28.451252\n\n\nFinally, we plot the data by site:\n\n# plot the data by site\nboxplot(feeding ~ site,\n        data = oystercatcher_r)\n\n\n\n\n\n\nFirst, we read in the data.\n\n# load the data\noystercatcher_py = pd.read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n# and have a look\noystercatcher_py.head()\n\n      site    feeding\n0  exposed  12.175506\n1  exposed  13.073917\n2  exposed  17.939687\n3  exposed  13.891783\n4  exposed  14.051663\n\n\nThe oystercatcher_py data set contains two columns:\n\na site column with information on the amount of shelter of the feeding location\na feeding column containing feeding rates\n\nNext, we get some basic descriptive statistics per group. Here we use the pd.groupby() function to group by site. We only want to have summary statistics for the feeding variable, so we specify that as well:\n\noystercatcher_py.groupby(\"site\")[\"feeding\"].describe()\n\n           count       mean       std  ...        50%        75%        max\nsite                                   ...                                 \nexposed     40.0  13.822899  2.441974  ...  13.946420  15.581748  18.560404\npartial     40.0  17.081666  2.619906  ...  16.927683  18.416708  23.021250\nsheltered   40.0  23.355503  2.419825  ...  23.166246  25.197096  28.451252\n\n[3 rows x 8 columns]\n\n\nFinally, we plot the data:\n\n# plot the data\n(\n  ggplot(oystercatcher_py,\n    aes(x = \"site\", y = \"feeding\"))\n    + geom_boxplot()\n)\n\n\n\n\n\n\n\nLooking at the data, there appears to be a noticeable difference in feeding rates between the three sites. We would probably expect a reasonably significant statistical result here."
  },
  {
    "objectID": "cs2_practical_anova.html#assumptions",
    "href": "cs2_practical_anova.html#assumptions",
    "title": "ANOVA",
    "section": "Assumptions",
    "text": "Assumptions\nTo use an ANOVA test, we have to make three assumptions:\n\nThe parent distributions from which the samples are taken are normally distributed\nEach data point in the samples is independent of the others\nThe parent distributions should have the same variance\n\nIn a similar way to the two-sample tests we will consider the normality and equality of variance assumptions both using tests and by graphical inspection (and ignore the independence assumption).\n\nNormality\nFirst we perform a Shapiro-Wilk test on each site separately.\n\ntidyverseRPython\n\n\nWe take the data, and group_by() site:\n\n# Shapiro-Wilk test on each site\noystercatcher %>% \n  group_by(site) %>% \n  shapiro_test(feeding)\n\n\n\nUnstack the data and perform a Shapiro-Wilk test on each group separately.\n\n# create a new object (a list) that contains the unstacked data\nuns_oystercatcher <- unstack(oystercatcher_r,\n                             form = feeding ~ site)\n# have a look at the data\nhead(uns_oystercatcher)\n\n   exposed  partial sheltered\n1 12.17551 15.20345  23.87176\n2 13.07392 16.53239  21.41133\n3 17.93969 13.64547  22.46681\n4 13.89178 23.02125  23.00843\n5 14.05166 20.39774  27.85932\n6 18.36498 14.03391  22.09400\n\n\nNext, we perform the Shapiro-Wilk test on each group:\n\nshapiro.test(uns_oystercatcher$exposed)\n\n\n    Shapiro-Wilk normality test\n\ndata:  uns_oystercatcher$exposed\nW = 0.98859, p-value = 0.953\n\nshapiro.test(uns_oystercatcher$partial)\n\n\n    Shapiro-Wilk normality test\n\ndata:  uns_oystercatcher$partial\nW = 0.98791, p-value = 0.9398\n\nshapiro.test(uns_oystercatcher$sheltered)\n\n\n    Shapiro-Wilk normality test\n\ndata:  uns_oystercatcher$sheltered\nW = 0.97511, p-value = 0.5136\n\n\n\n\nWe use the pg.normality() function to calculate the statistic. This requires:\n\nthe dv dependent variable (feeding in our case)\nthe group variable (site)\nand some data\n\n\npg.normality(dv = \"feeding\",\n             group = \"site\",\n             data = oystercatcher_py)\n\n                  W      pval  normal\nexposed    0.988587  0.953037    True\npartial    0.987907  0.939830    True\nsheltered  0.975106  0.513547    True\n\n\n\n\n\nWe can see that all three groups appear to be normally distributed which is good.\nFor ANOVA however, considering each group in turn is often considered quite excessive and, in most cases, it is sufficient to consider the normality of the combined set of residuals from the data. We’ll explain residuals properly in the next session but effectively they are the difference between each data point and its group mean. The residuals can be obtained directly from a linear model fitted to the data.\nSo, we create a linear model, extract the residuals and check their normality:\n\ntidyverseRPython\n\n\n\n# define the model\nlm_oystercatcher <- lm(feeding ~ site,\n                       data = oystercatcher)\n\n# extract the residuals\nresid_oyster <- residuals(lm_oystercatcher)\n\n# perform Shapiro-Wilk test on residuals\nresid_oyster %>% \n  shapiro_test()\n\n# A tibble: 1 × 3\n  variable statistic p.value\n  <chr>        <dbl>   <dbl>\n1 .            0.994   0.857\n\n\n\n\n\n# define the model\nlm_oystercatcher_r <- lm(feeding ~ site,\n                         data = oystercatcher_r)\n\n# extract the residuals\nresid_oyster_r <- residuals(lm_oystercatcher_r)\n\n# perform Shapiro-Wilk test on residuals\nshapiro.test(resid_oyster_r)\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid_oyster_r\nW = 0.99355, p-value = 0.8571\n\n\n\n\nUnfortunately pingouin does not have a straightforward way of extracting residuals (if you know more, please raise an issue!).\nSo we need to do some statistical gymnastics to get the residuals. I would suggest that you don’t worry about the code, just run it and look at the output!\n\n\n\n\n\n\nTechnical details (optional)\n\n\n\n\n\nFor this to work in Python, we need to take our predictor variable (site) and convert it to a dummy variable.\nWe also need to import the statsmodels.api module, which contains an OLS() function (Ordinary Least Squares - the equivalent of the lm() function in R).\nWe also import formula.api so we can use the formula notation in our linear model. We define the formula as formula= \"feeding ~ C(site)\" with C conveying that the site variable is a category. Lastly we can .fit() the model.\nIf you’re familiar with this stuff then you can look at the model itself by running summary(lm_oystercatcher_py). But we’ll cover all of this in later sessions.\n\n\n\nEventually, we get the residuals from the linear model with .resid. We give this to the stats.shapiro() function and off we go…\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# fitting the model\n\nlm_oystercatcher_py = smf.ols(formula= \"feeding ~ C(site)\", data = oystercatcher_py).fit()\n\nstats.shapiro(lm_oystercatcher_py.resid)\n\nShapiroResult(statistic=0.993545651435852, pvalue=0.8570298552513123)\n\n\n\nimport statsmodels.formula.api as smf\n\n# fitting the model\n\nmodel = smf.ols(formula= \"feeding ~ C(site)\", data = oystercatcher_py)\nresult = model.fit()\n\nresult.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:         feeding       R-squared:             0.720\n\n\n  Model:                   OLS         Adj. R-squared:        0.716\n\n\n  Method:             Least Squares    F-statistic:           150.8\n\n\n  Date:             Sat, 06 Aug 2022   Prob (F-statistic): 4.13e-33\n\n\n  Time:                 14:21:19       Log-Likelihood:      -278.49\n\n\n  No. Observations:         120        AIC:                   563.0\n\n\n  Df Residuals:             117        BIC:                   571.3\n\n\n  Df Model:                   2                                    \n\n\n  Covariance Type:      nonrobust                                  \n\n\n\n\n                          coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept               13.8229     0.395    35.032  0.000    13.041    14.604\n\n\n  C(site)[T.partial]       3.2588     0.558     5.840  0.000     2.154     4.364\n\n\n  C(site)[T.sheltered]     9.5326     0.558    17.083  0.000     8.427    10.638\n\n\n\n\n  Omnibus:        0.490   Durbin-Watson:         2.126\n\n\n  Prob(Omnibus):  0.783   Jarque-Bera (JB):      0.642\n\n\n  Skew:           0.107   Prob(JB):              0.726\n\n\n  Kurtosis:       2.713   Cond. No.               3.73\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nAgain, we can see that the combined residuals from all three groups appear to be normally distributed (which is as we would have expected given that they were all normally distributed individually!)\n\n\nEquality of Variance\nWe now test for equality of variance using Bartlett’s test (since we’ve just found that all of the individual groups are normally distributed).\nPerform Bartlett’s test on the data:\n\ntidyverseRPython\n\n\n\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  feeding by site\nBartlett's K-squared = 0.29598, df = 2, p-value = 0.8624\n\n\nWhere the relevant p-value is given on the 3rd line. Here we see that each group appears to have the same variance.\n\n\n\n# check equality of variance\nbartlett.test(feeding ~ site,\n              data = oystercatcher)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  feeding by site\nBartlett's K-squared = 0.29598, df = 2, p-value = 0.8624\n\n\nWhere the relevant p-value is given on the 3rd line. Here we see that each group appears to have the same variance.\n\n\nWe use the homoscedasticity() function from pingouin (homoscedasticity is another way of describing equality of variance). The default method is levene, so we need to specify that we want to use bartlett.\n\npg.homoscedasticity(dv = \"feeding\",\n                    group = \"site\",\n                    method = \"bartlett\",\n                    data = oystercatcher_py)\n\n                 T      pval  equal_var\nbartlett  0.295983  0.862439       True\n\n\nWhere the relevant p-value is given in the pval column. Here we see that each group appears to have the same variance.\n\n\n\n\n\nGraphical interpretation and diagnostic plots\nR provides a convenient set of graphs that allow us to assess these assumptions graphically.\n\ntidyverseR\n\n\nIn the first session we already created diagnostic Q-Q plots directly from our data, using stat_qq() and stat_qq_line(). For more specific plots this becomes a bit cumbersome. There is an option to create ggplot-friendly diagnostic plots, using the ggResidPanel package.\nLet’s create the diagnostic plots we’re interested in using ggResidPanel:\n\nlm_oystercatcher %>% \n    resid_panel(plots = c(\"resid\", \"qq\", \"ls\", \"cookd\"),\n              smoother = TRUE)\n\n\n\n\n\nThe top left graph plots the Residuals plot. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal blue line (and if there are sufficient points then the red line, which is a smoother line, should be on top of the blue line). This plot looks pretty good.\nThe top right graph shows the Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal blue line. This plot looks good.\nThe bottom left Location-scale graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe last graph shows the Cook’s distance and tests if any one point has an unnecessarily large effect on the fit. A rule of thumb is that if any value is larger than 1.0, then it might have a large effect on the model. If not, then no point has undue influence. This plot is good. There are different ways to determine the threshold (apart from simply setting it to 1) and in this plot the blue dashed line is at 4/n, with n being the number of samples. At this threshold there are some data points that may be influential, but I personally find this threshold rather strict.\n\n\n\nWe can create the default diagnostic plots in base R by simply plotting the lm object.\n\n# create a neat 2x2 window\npar(mfrow = c(2,2))\n# create the diagnostic plots\nplot(lm_oystercatcher)\n\n\n\n# and return the window back to normal\npar(mfrow = c(1,1))\n\nThe second line creates four diagnostic plots.\n\nThe top left graph plots the Residuals vs Fitted. If the data are best explained by a linear line then there should be a uniform distribution of points above and below the horizontal red line. This plot looks pretty good.\nThe top right graph shows the Normal Q-Q plot which allows a visual inspection of normality. If the residuals are normally distributed, then the points should lie on the diagonal dotted line. This plot looks good.\nThe bottom left Scale-Location graph allows us to investigate whether there is any correlation between the residuals and the predicted values and whether the variance of the residuals changes significantly. If not, then the red line should be horizontal. If there is any correlation or change in variance then the red line will not be horizontal. This plot is fine.\nThe bottom right Residuals vs Factor Levels plot shows the residuals for each group (= site). This plot is only displayed if there are equal number of observations in each group and we’ll explain more about this in the next session.\n\n\n\n\nWe can see that these graphs are very much in line with what we’ve just looked at using the test, which is reassuring. The groups all appear to have the same spread of data, and the Q-Q plot shows that the assumption of normality is alright.\n\n\n\n\n\n\nAssessing assumptions\n\n\n\nAt this stage, I should point out that I nearly always stick with the graphical method for assessing the assumptions of a test. Assumptions are rarely either completely met or not met and there is always some degree of personal assessment.\nWhilst the formal statistical tests (like Shapiro-Wilk) are technically fine, they can often create a false sense of things being absolutely right or wrong in spite of the fact that they themselves are still probabilistic statistical tests. In these exercises we are using both approaches whilst you gain confidence and experience in interpreting the graphical output and whilst it is absolutely fine to use both in the future I would strongly recommend that you don’t rely solely on the statistical tests in isolation."
  },
  {
    "objectID": "cs2_practical_anova.html#implement-test",
    "href": "cs2_practical_anova.html#implement-test",
    "title": "ANOVA",
    "section": "Implement test",
    "text": "Implement test\nPerform an ANOVA test on the data:\n\nanova(lm_oystercatcher)\n\nThe first line fits a linear model to the data (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which I’ve called lm_oystercatchers, but which you can call what you like). The second line actually carries out the ANOVA analysis.\n\nThe first argument must be in the formula format: response ~ predictor\nIf the data are stored in stacked format, then the second argument must be the name of the data frame\nThe anova() command takes a linear model object as its main argument"
  },
  {
    "objectID": "cs2_practical_anova.html#interpret-output-and-report-results",
    "href": "cs2_practical_anova.html#interpret-output-and-report-results",
    "title": "ANOVA",
    "section": "Interpret output and report results",
    "text": "Interpret output and report results\nThis is the output that you should now see in the console window:\n\n\nAnalysis of Variance Table\n\nResponse: feeding\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nsite        2 1878.02  939.01  150.78 < 2.2e-16 ***\nResiduals 117  728.63    6.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say “that it is very unlikely that these three samples came from the same parent distribution” and as such we can reject our null hypothesis and state that:\n\nA one-way ANOVA showed that the mean feeding rate of oystercatchers differed significantly between locations (F = 21.51, df = 2, 12, p = 0.00011).\n\nNote that we have included (in brackets) information on the test statistic (F = 21.51), both degrees of freedom (df = 2, 12), and the p-value (p = 0.00011)."
  },
  {
    "objectID": "cs2_practical_anova.html#implement-and-interpret-the-test",
    "href": "cs2_practical_anova.html#implement-and-interpret-the-test",
    "title": "ANOVA",
    "section": "Implement and interpret the test",
    "text": "Implement and interpret the test\nPerform an ANOVA test on the data:\n\ntidyverseRPython\n\n\n\nanova(lm_oystercatcher)\n\nAnalysis of Variance Table\n\nResponse: feeding\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nsite        2 1878.02  939.01  150.78 < 2.2e-16 ***\nResiduals 117  728.63    6.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we’ve called lm_oystercatcher, but which you can call what you like).\nIn the output:\n\nThe 1st line just tells you the that this is an ANOVA test\nThe 2nd line tells you what the response variable is (in this case feeding)\nThe 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\n\nThe Df column contains the degrees of freedom values on each row, 2 and 117 (which we can use for the reporting)\nThe F value column contains the F statistic, 150.78 (which again we’ll need for reporting).\nThe p-value is 2.2e-16 and is the number directly under the Pr(>F) on the 4th line (to be precise, it is 4.13e-33 but anything smaller than 2.2e-16 gets reported as < 2.2e-16).\nThe other values in the table (in the Sum Sq and Mean Sq) columns are used to calculate the F statistic itself and we don’t need to know these.\n\nThe 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a *** symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a * character next to it, etc. Thankfully we can all cope with actual numbers and don’t need a short-hand code to determine the reporting of our experiments (please tell me that’s true…!)\n\n\n\n\nanova(lm_oystercatcher_r)\n\nAnalysis of Variance Table\n\nResponse: feeding\n           Df  Sum Sq Mean Sq F value    Pr(>F)    \nsite        2 1878.02  939.01  150.78 < 2.2e-16 ***\nResiduals 117  728.63    6.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis takes the linear model (i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis) and stores this information in an R object (which we’ve called lm_oystercatcher_r, but which you can call what you like).\nIn the output:\n\nThe 1st line just tells you the that this is an ANOVA test\nThe 2nd line tells you what the response variable is (in this case feeding)\nThe 3rd, 4th and 5th lines are an ANOVA table which contain some useful values:\nThe Df column contains the degrees of freedom values on each row, 2 and 117 (which we can use for the reporting)\nThe F value column contains the F statistic, 150.78 (which again we’ll need for reporting).\nThe p-value is 2.2e-16 and is the number directly under the Pr(>F) on the 4th line (to be precise, it is 4.13e-33 but anything smaller than 2.2e-16 gets reported as < 2.2e-16).\nThe other values in the table (in the Sum Sq and Mean Sq) columns are used to calculate the F statistic itself and we don’t need to know these.\nThe 6th line has some symbolic codes to represent how big (small) the p-value is; so, a p-value smaller than 0.001 would have a *** symbol next to it (which ours does). Whereas if the p-value was between 0.01 and 0.05 then there would simply be a * character next to it, etc. Thankfully we can all cope with actual numbers and don’t need a short-hand code to determine the reporting of our experiments (please tell me that’s true…!)\n\n\n\nThere are different ways of conducting an ANOVA in Python, with scipy.stats proving an option. However, I find that anova() function in pingouin provides the easiest and most-detailed option to do this.\nIt takes the following arguments:\n\ndv: dependent variable (response variable; in our case feeding)\nbetween: between-subject factor (predictor variable; in our case site)\ndata: which function doesn’t!?\ndetailed: optional True or False, we’re setting it to True because we like to know what we’re doing!\n\n\npg.anova(dv = \"feeding\",\n         between = \"site\",\n         data = oystercatcher_py,\n         detailed = True)\n\n   Source           SS   DF          MS           F         p-unc       np2\n0    site  1878.015371    2  939.007685  150.782449  4.128088e-33  0.720473\n1  Within   728.625249  117    6.227566         NaN           NaN       NaN\n\n\nThis creates a linear model based on the data, i.e. finds the means of the three groups and calculates a load of intermediary data that we need for the statistical analysis.\nIn the output:\n\nSource: Factor names - in our case these are the different sites (site)\nSS: Sums of squares (we’ll get to that in a bit)\nDF: Degrees of freedom (at the moment only used for reporting)\nMS: Mean squares\nF: Our F-statistic\np-unc: p-value (unc stands for “uncorrected” - more on multiple testing correction later)\nnp2: Partial eta-square effect sizes (more on this later)\n\n\n\n\nAgain, the p-value is what we’re most interested in here and shows us the probability of getting samples such as ours if the null hypothesis were actually true.\nSince the p-value is very small (much smaller than the standard significance level of 0.05) we can say “that it is very unlikely that these three samples came from the same parent distribution” and as such we can reject our null hypothesis and state that:\n\nA one-way ANOVA showed that the mean feeding rate of oystercatchers differed significantly between locations (F = 150.78, df = 2, 117, p = 4.13e-33)."
  },
  {
    "objectID": "area_51.html",
    "href": "area_51.html",
    "title": "area_51",
    "section": "",
    "text": "from dgplots import *\n\n\noystercatcher_py = pd.read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n\n\n# define the model\nmodel = smf.ols(formula= \"feeding ~ C(site)\", data = oystercatcher_py)\n# fit the model\nresults = model.fit()\n\n\ndgplots(results)\n\n\n\n\n\n\n\nstate_py = pd.read_csv(\"data/CS3-statedata.csv\")\n\n\n# define the model\nmodel = smf.ols(formula= \"population ~ area + murder\", data = state_py)\n# fit the model\nlm_state = model.fit()\n\n\ndgplots(lm_state)"
  },
  {
    "objectID": "area_51.html#q-q-plot",
    "href": "area_51.html#q-q-plot",
    "title": "area_51",
    "section": "Q-Q plot",
    "text": "Q-Q plot\n\n\n\n\n\n<ggplot: (346219133)>"
  },
  {
    "objectID": "area_51.html#residual-plot",
    "href": "area_51.html#residual-plot",
    "title": "area_51",
    "section": "Residual plot",
    "text": "Residual plot\n\n\n\n\n\n<ggplot: (346229637)>"
  },
  {
    "objectID": "area_51.html#location-scale-plot",
    "href": "area_51.html#location-scale-plot",
    "title": "area_51",
    "section": "Location-Scale plot",
    "text": "Location-Scale plot\n\n\n\n\n\n<ggplot: (346334413)>"
  },
  {
    "objectID": "area_51.html#cooks-distance",
    "href": "area_51.html#cooks-distance",
    "title": "area_51",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\n\n\n\n\n<ggplot: (346359772)>"
  },
  {
    "objectID": "dplots_nb.html",
    "href": "dplots_nb.html",
    "title": "Core statistics",
    "section": "",
    "text": "oystercatcher_py = pd.read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n# define the model\nmodel = smf.ols(formula= \"feeding ~ C(site)\", data = oystercatcher_py)\n# fit the model\nresults = model.fit()\n# get relevant variables from model\nresiduals = results.resid.rename(\"residuals\")\nfitted_values = results.fittedvalues.rename(\"fitted_values\")\nstd_resid = pd.Series(results.resid_pearson).rename(\"std_resid\")\ninfluence = results.get_influence()\ncooks_d = pd.Series(influence.cooks_distance[0]).rename(\"cooks_d\")\nleverage = pd.Series(influence.hat_matrix_diag).rename(\"leverage\")\nobs = pd.Series(range(len(residuals))).rename(\"obs\")\nn_obs = len(obs.index)\n\n# combine Series into DataFrame\nmodel_values = residuals.to_frame().join(fitted_values).join(std_resid).join(cooks_d).join(leverage).join(obs)\n\nmodel_values[\"n_obs\"] = n_obs\n\n(\n  ggplot(model_values, aes(sample = \"residuals\"))\n  + stat_qq()\n  + stat_qq_line(colour = \"blue\")\n)\n\n\n\n\n<ggplot: (374354479)>\n\n\n\ng1 = (ggplot(mtcars) + geom_point(aes(\"mpg\", \"disp\")))\ng2 = (ggplot(mtcars) + geom_boxplot(aes(\"gear\", \"disp\", group=\"gear\")))\ng3 = (ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)')) + geom_point() + stat_smooth(method='lm') + facet_wrap('~gear'))\ng4 = (ggplot(data=diamonds) + geom_bar(mapping=aes(x=\"cut\", fill=\"clarity\"), position=\"dodge\"))\n\ng1 = pw.load_ggplot(g1, figsize=(2,3))\ng2 = pw.load_ggplot(g2, figsize=(2,3))\ng3 = pw.load_ggplot(g3, figsize=(3,3))\ng4 = pw.load_ggplot(g4, figsize=(5,2))\ng1234 = (g1|g2|g3)/g4\ng1234.savefig()\n\n\n\n\n\ng1234.savefig()"
  },
  {
    "objectID": "cs2_practical_anova.html",
    "href": "cs2_practical_anova.html",
    "title": "ANOVA",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "test_nb.html",
    "href": "test_nb.html",
    "title": "Core statistics",
    "section": "",
    "text": "from dgplots import *\noystercatcher_py = pd.read_csv(\"data/CS2-oystercatcher-feeding.csv\")\n# define the model\nmodel = smf.ols(formula= \"feeding ~ C(site)\", data = oystercatcher_py)\n# fit the model\nresults = model.fit()\n\ndgplots(results)"
  }
]